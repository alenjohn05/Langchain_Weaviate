[
    {
        "speaker": "Connor Shorten",
        "content": "Hey, everyone. Thank you so much for watching the Weaviate podcast. I'm super excited to welcome Aleksa Gordcic. Aleksaa is one of the most well-known AI, I don't want to say influencers, because of how the term has evolved. Now, I mean, like here are 10 ways chatGPT can make you a million dollars. But Aleksa, I like to think of this cohort of, I guess, myself, Aleksa, Yannic, Tim, and Letita. There was this time where we were making deep learning paper summary videos on YouTube, and it felt like we were kind of the cohort doing this. And it was a really fun kind of time in the evolution of my career. And so, always been like, what Aleksa is doing, and recently his story with joining DeepMind from that, and then leaving DeepMind to start Ortus. This whole thing is so inspiring. I love how you have these YouTube videos where you, hey, I'm in Japan, and I've just got out of Y Combinator, and I'm here hacking all night. I love the storytelling of it. It's so much fun. So, before going any further, Aleksaa, thank you so much for joining the podcast. ",
        "podNumber": 55,
        "clipNumber": 1
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Thanks a lot for the beautiful introduction. And yeah, please don't call me an influencer. It definitely has a negative connotation in my mind as well. ",
        "podNumber": 55,
        "clipNumber": 2
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, well, I do want to give you some credit into the technical chops that you have quickly on this YouTube thing. So, playing this game with you and Yannic, and we were in this race to summarize a paper, try to get as much attention for this summary as you can, to build a YouTube channel and all this. And I think a couple of things that you did that really stood out to me as watching what you were doing across the fence and competing with you as well, seeing like, what can I steal from this guy?",
        "podNumber": 55,
        "clipNumber": 3
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "I never considered it a competition, to be honest. I think that the space was big enough, it's a global scene, and there were only a couple of us. So, I really never considered it a competition. And I'm also very bullish on Peter Thiels \u201ccompetition is for losers\u201d. So, yeah, I don't know. Yeah, just a comment. ",
        "podNumber": 55,
        "clipNumber": 4
    },
    {
        "speaker": "Connor Shorten",
        "content": "I agree with that super strongly. I think with a lot of things, markets are underserved, and competition\u2026 I don't think of it as only one of us can win, but even with vector databases, I think of it as like, what are these other people doing? And competition, I think, is a big force that shows you these ideas. And it would be foolish to just not even look at the other people doing the thing. But two things I want to really give you credit for is, first, I remember when you started diving into the graph neural network stuff, you did some really impressive paper summaries. And to kind of read a couple of them, the neural sheath diffusion one, as well, sorry, my thing, I moved somewhere. ",
        "podNumber": 55,
        "clipNumber": 5
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Yeah, that one was very hard. And that was not in the early days. In the first batch of my graph ML videos, it was more like the popular ones, like the graph attention network from Petro Velichovic, graph convolutional networks from Thomas Kipf, a couple of those, I guess, with higher number of citations. And then this one came, I think, way after that first wave of the initial series. But yeah, that one was super tough. And if you ask me now the details about the paper, I haven't done it in a while, so it would be very hard to recall all the details. But I did understand it back then. ",
        "podNumber": 55,
        "clipNumber": 6
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. Well, yeah, I mean, it was such an impressive, I mean, because graph neural networks, you know, this kind of like manifold learning and like where you're processing meshes, it's such a complex, I think, you know, it's a really impressive thing. And also, sorry, I'm noticing that I'm trying out a new camera. So for our listeners, I usually nod along a lot when I'm listening to people speak, and this camera is following my head. So I'm going to try not to do that to keep this camera in place. But another thing you did was, I remember you did this explanation of Jina AI, you had done this video, I think it was like a half hour video where you just went through the entire code base. And for the time, because, you know, most of us in this cohort, we're just summarizing papers. And I think you showed this kind of dynamic of like, you know, I understand the code base as well as how to read these research papers. Can you talk me through like the evolution in your career when you went just like that decision when it was like, today, I'm going to walk through the Jina code base. ",
        "podNumber": 55,
        "clipNumber": 7
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "I don't remember exactly like how that came to be. But like, I knew, like, I really wanted something a bit like more interesting, something novel, that's not just papers also just to give myself a little bit of break and find some some joy in some other types of videos. And so that one, like I know, the Jina AI folks actually reached out to me and asked whether I could do something like that. And I actually wanted, I didn't know whether that was my first video, remember, but I know that later, I had very, very like deep dives, like hour and a half, two hours long of like whisper code base. And I don't even remember all of all of the ones that I kind of covered. But it is, as you said, it's very complimentary. Like I realized that early early on in my ML career that if I after I read the paper, if I check out the code, sometimes a single equation creates me like much more like denser mental model of what's going on in the paper than just reading the whole thing and like being confused oftentimes. And so I knew that's a great way for me to learn. And also, I wanted to see what other people are going to enjoy that because like, I was literally stepping through the code, like line by line by line, explaining every single variable. It was also, I guess, even for beginners, they could probably follow because I was explaining even basic Python stuff, not just like hardcore ML stuff. But yeah, don't exactly remember why I made that first one, but I'm happy I did. ",
        "podNumber": 55,
        "clipNumber": 8
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, well, super cool. So I think all that just, you know, paying a quick tribute to your YouTube career, which, you know, that's how I first came to know you. And I thought all that was just so great. And so then coming into DeepMind, take me through like the, you know, the early days, your maybe initial expectation. I mean, DeepMind is like, you know, Stanford, MIT, OpenAI, I don't know, it is, it's DeepMind, it's the biggest thing in the world. So like, what were your initial thoughts? ",
        "podNumber": 55,
        "clipNumber": 9
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "You mean, why did I choose DeepMind? Or what were my initial, like, how did the preps look like? Or what in particular are you curious about? ",
        "podNumber": 55,
        "clipNumber": 10
    },
    {
        "speaker": "Connor Shorten",
        "content": "I'm really curious about kind of mentally, like, okay, so you, I think, like this decision to, I don't know what you're, in addition to when you're doing these YouTube videos, what you were maybe additionally doing, or if that was the only thing you were doing, but that kind of evolution of like, now I'm in DeepMind, did you feel like you just got drafted into the NBA? ",
        "podNumber": 55,
        "clipNumber": 11
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Like, yeah, I already felt fairly comfortable, to be honest, like, because the journey started, I guess, it was the inception, like, happened back in 2018. That was the first time where I attended this machine learning summer camp organized by Microsoft folks. And there were a couple of at least one lecture from DeepMind. And all of them were like, fellow Serbians. And so back then, I was like, Oh, my God, this is this is kind of those guys like AlphaGo and the full fame. And like, I really want to be there, I want to be at the forefront of where people are building exciting AI stuff. And so that's when the inception, like happened, if that's how I can phrase it. And then a couple of years later, like I really started learning, like getting into much more depth. And papers were definitely instrumental to getting there, because I was doing kind of breadth first search across all these different areas, and also kind of going in depth as much as possible and coding stuff from scratch. And so that really helped me ground and have some decent understanding of various different, different subfields and couple that with my software engineering skills and like, background in electrical engineering, I was already fairly grounded, so to speak. So, so yeah, for like a research engineering role, you don't obviously don't have to be like PhD, postgrad or something like in depth. And so it was, I guess, enough for me to land the job. ",
        "podNumber": 55,
        "clipNumber": 12
    },
    {
        "speaker": "Connor Shorten",
        "content": "So I was, I'm always really curious about how this could like, do you have a particular niche of deep learning that you enjoy the most? Like, I know, with the paper summary things, you have to kind of cover a lot, like, you know, audio clip, and then DALLE, Dino, I'm looking at your channel on the website, like you cover such a breadth of the topics, and I feel like you kind of have to do that to make the YouTube channel to hit a decent audience if you just narrowed into active learning or something like that, right? But when you got to DeepMind, was it still very breadth first? Or was I imagine it's quite like, yeah.",
        "podNumber": 55,
        "clipNumber": 13
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "For me, to be honest, because in that period, many of those fields were that was the first time I ever started reading papers or anything about those fields. So it was not really with with audience in mind, to be honest, it was fairly selfish, I wanted to do like, I wanted to have a decent understanding of all of these fields. So I can create cross connections and, and like, transfer ideas from one field to another field. And so to get to your question, even even like, when the last preps for the final started, I did not converge into one single field, although at Microsoft, I was in computer vision, like my role, because I was working on the HoloLens. And so I did work more with convolutional neural networks and such things. But, but yeah, I wouldn't say I ever converged too much into any any particular subfield, maybe computer vision kind of is standing out a little bit. ",
        "podNumber": 55,
        "clipNumber": 14
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, super interesting. So yeah, so I'm gonna ask you some questions later about the new Apple headset and HoloLens. And I'm sure you'll have an interesting perspective on that. But let's dive into it. Ortus, the decision to leave deep mind, start Ortus, what is the what is the compelling vision that just made you say, I have to do this? Like, I can't live another day of working on this. ",
        "podNumber": 55,
        "clipNumber": 15
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Yeah, so so for me, like, even before I joined deep mind, and not even deep mind, like Microsoft, like I knew I want to ultimately build my own like startup, that's the same reason I dropped out of my master's study as well. Like I, I like, I'm not the person who operates super well under these structured environments where you have so much levels of supervision above you and people telling you what to do. I love I love this unsupervised space, so to speak. And like, and because of that, like character of mine, and the way I learned, I think the most natural thing for me was to decide, yeah, I want to early on, I decided I want to build my own company. And then I was like, okay, let's first get some get some experience in the in the real world, so to speak, in the in the industry. And then so came Microsoft and DeepMind. So yeah, I, this has been a long way of saying this was like a very, very something I've been planning for for a while. And, and then getting back to to to Ortus, I guess it kind of connects a lot of my different passions. One of them is obviously the thing we spoke about a lot of times so far, and that's that's the YouTube career. Like I really like being close to people and users like the B2C component is kind of attractive. And, and then there is the whole multimodal space. And like the interesting stuff happening with large language models and as well as like, well, multimodal models in general. And so like understanding videos, which is what Ortiz is kind of about, you ask a question, and you get immediately you're watching a video just for your audience, for people who probably never heard of it. You basically open up a YouTube video currently is just like a small Chrome extension. And you can type in a question and ask, hey, what's going on in this video? What what did they talk about? And then you get an instant reply, you get where that thing is happening the video, you get some suggested questions or summaries, there's a bunch of details that help you basically more search more efficiently of what's going on in that video content. And, and yeah, and like one thing probably worth mentioning is that the Chrome extension is obviously not the end goal. Like, I do have a bigger vision, which I will not disclose at this point of time. But I'm really enjoying the learning and I think it's an interesting space.",
        "podNumber": 55,
        "clipNumber": 16
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think and the connection you're drawing between vision and language and your unique expertise in computer vision, I really want to come back to that. But this is why I kind of as I was designing these discussion topics really wanted to open with kind of your YouTube background. And now we have all these new tools with obviously like Weaviate and these LLMs to build these search indexes. So first, before we dive a little more into maybe just describing the stack behind Ortus and how you produce summaries, how you build question answering systems, let's like we could continue on your background in you know, making paper summaries deep, deeply, you know, I think your channel is like nearly 50,000 subscribers, which you don't get to without obsessing over how to do it. So how do you think like, the whole content creators space is going to evolve with these new technologies? ",
        "podNumber": 55,
        "clipNumber": 17
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Yeah, getting back to your subscribers mentioned, I think, I think it's actually super easy to get 50k or more, if you decide to optimize for that. And if you just like, go a bit more higher level, like, for example, if the two of us were just to start covering Python, PyTorch, TensorFlow, latest trends, latest stuff in AI news, all of that is super trendy and popular. And like, I know that that content goes very well, I can just see by the views on my channel as well. And types of video where I described, hey, how was my experience with Microsoft DeepMind, all of those are very, very, like popular, yet I'm not doubling down on that. Just because like, I also have selfish interests with my YouTube channel. I never wanted to be a YouTuber. And I never considered myself a YouTuber. For me, YouTube was an outlet for me to learn stuff. And at the same time, share with the community and build some like, for a community of passionate ML folks. So that's how I think about it, honestly. And then what was your question? Actually, I kind of went on a tangent. I never came back. ",
        "podNumber": 55,
        "clipNumber": 18
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think that's the right way of thinking about it, though, is building the community is the right way to think about it, you know, but my question is, now using Ortus, you can basically turn the AI epiphany into a chatbot that you know, people can, I think there are kind of two things to this. There's firstly, yeah, the chatbot and the summaries. I liked what you said on Ken Jee's podcast about how long form content shouldn't be consumed sequentially. I agree. Like this podcast is like, we're going to talk about so many different topics. It's like, what's relevant to you just watching it sequentially, probably not the best way to do it. So and then if I could just add one third thing to that is the multilingual translation part. I think there's a huge potential in that with these kind of things. So just like, how do you think the experience for content consumption is about? I think it's about to totally transform. ",
        "podNumber": 55,
        "clipNumber": 19
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Oh, yeah, 100%. I mean, like, I like to think that Ortus is kind of trying to demonstrate some of those possibilities at the moment. And you also mentioned, like, languages, like it's so easy with powerful large language models, which, because we know that the corpus that they were trained on contains in some percentages, various languages. And so it's actually not not that hard to to add multilingual support, I would say. Although the quality would probably be like dropping in one way or another, which I can't quantify right now. But but yeah, definitely, definitely super exciting time, because for the first time, like we do this fairly complex medium, right? Like just just from the standpoint of, like the memory and the computing power it takes for you to process that. And so historically, we were not really good at understanding what's going on in the videos. And like, even the transcription was not that great, right? Like until Whisper, like came out, like I think people were struggling, even with transcription, which is just like text, ignoring the whole visual space. And then, and then when you go like, even which, like maybe with shorter videos, like 30 seconds or something, YouTube and other platforms could use some heuristics to analyze the visual components. But like, it's nothing more than heuristics. And I think we're finally getting to the point where we'll be able to use models, like maybe Flamingo, maybe maybe like GPT-4, we're still waiting for the for the image feature to drop. And yeah, I think, I think it's going to be very interesting to see how this space evolves and what we can do with like analysis of video content. ",
        "podNumber": 55,
        "clipNumber": 20
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think like, like when Stanford Engineering, like Chelsea Finn's lectures on multi-task learning and meta learning, it's like, I'm so interested in that. But then it's like, or like Andy Pavlo's database course from Carnegie Mellon, it's like, I love, like I'm so interested in it, but I can't just 25 hours of sitting, I don't have the attention span for it, honestly. Like, it's quite hard. So yeah, this, can you tell me about how Ortus, like just walk me through it? I'm a user and, you know, how, what can we do to make, you know, these courses easier to digest? ",
        "podNumber": 55,
        "clipNumber": 21
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "So are you asking for like a brief digest of how the thing works, or just like how it looks like from the user perspective? Yeah, I suppose just like, hopefully the question's not too redundant from what we're already talking about, but just kind of like any general vision on how we can take these 25 hour courses and just, I don't know, create a more personalized experience. Just build that layer on top of that, that makes it easier to process it. Maybe like if I could set the stage more with how I'm thinking about it is like, if you cut it up into clips, like you use the language models to automatically identify chapters, and then you kind of like reorganize it already to convert it instead of just the sequential flow, that's kind of one idea. Just like the chatbot idea, which kind of gives you something to, you know, like, have your questions answered in a pretty novel way. And I suppose that general question, and we can move on if it's not. ",
        "podNumber": 55,
        "clipNumber": 22
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "No, I think, well, like long term, like, I think, ideally, we'll just have like a big neural network. And we'll just pass like video frames through that neural network, maybe sub-sample using some saliency heuristics, not just like dummy uniform sampling or something. And then it just spits out the answer. I think, unless the video is like super, super long, I think that these super big neural networks will be able to do that. And so I think we are currently in this, maybe transitionary stage where we are doing these hybrid systems. But I actually do believe that long term, we'll see more paradigm shifts. I'm not sure about that. Maybe, like maybe that's kind of how I see it. It's still prohibitively expensive to do what they just said. And I definitely think that things such as external memories, one of those examples being vector databases and Weaviate, do have a place to play because ultimately, we know that that's the weak point of these systems. Same as with humans, right? Like, I think we shouldn't dismiss just because it's like not neural network, not biology inspired, we should definitely not like ditch those ideas. I think some of the papers, like, was it neural turing machines? There was one of the early ones from Alex Graves, if I'm not mispronouncing his name. That was one of the early papers that really sparked my imagination for these types of hybrid systems where you're combining external memory. And then later, we had retrievals, like retro from DeepMind or some of those ideas. So that's going to be a component. But I wouldn't be surprised if we just had fewer networks that could analyze the video, like just in a single forward pass or something. ",
        "podNumber": 55,
        "clipNumber": 23
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, jI think kind of separating it a bit, the idea of like, vision language model that just like watches a video that I think that's a pretty exciting idea. I know, like with Flamingo, and I think there's recently a new model from Character AI, that's like, they're making a lot of progress in this thing. And I think that's a hard thing for people even to wrap their heads, we're already still kind of like, recovering from the hangover of chatGPT, it's already so much that it's like, still, I think, wrapping our heads around what we can even do with this, like, just, you know, like, I'm curious, like, what you think about, like, you know, LangChain, LlamaIndex, just these kind of ideas of like, taking the chatGPT into an API, and then plugging it into all this data. And it seems to me like that's still just, even before the Flamingo stuff gets better, and just flips everything on its head again. I feel like there's still a lot. ",
        "podNumber": 55,
        "clipNumber": 24
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "No, I think that's super powerful. Like, we've seen the adoption of Lang chain, and it's definitely democratizing, like machine learning for a lot of developers. And we are seeing more, I think, in the app development space, one could even argue that, like, if you're a great web developer, if you're just great software engineer, you might have like an advantage, compared to ML engineers. And obviously, like later, when you want to optimize for if you want to reduce the cost, if you want to find your neural models, if you want to do all of that, that's where the ML expertise comes into the picture. But for the initial stages, we're just trying to, like, roll out an MVP or something, like being a great software engineer or web developer might be like, even like a better than being an ML engineer, if that makes sense. But yeah, like all of those LangChain, LlamaIndex, like Weaviate, Pinecone, all of those systems are definitely currently, with this current technology, with the levels where we are, they are, they are, they are being the shovels for this for this gold rush. And, and I think there's a lot of value that's gonna like be acquired by the humanity over over the next couple of years. But I wouldn't bet on the in the long run, I'm not sure how this is going to roll out, to be honest. I think that we are, we're like kind of getting into this software 1.0 mindset with some of these frameworks, which is not necessarily bad. But like, we went from pure, we were just like deep learning, everything is going to be just deep learning, deep learning. And then there was Gary Marcus, who was like, no, it's going to be hybrid systems. And then all of a sudden, it is hybrid systems, which is not super surprising, even when people who were very bullish on deep learning knew that eventually, as I said, you like for perfect memory, it's much better to have actual external memory other than like, let the network store the memory, the coefficients. So yeah, I'm gonna stop here. I was rambling too much. ",
        "podNumber": 55,
        "clipNumber": 25
    },
    {
        "speaker": "Connor Shorten",
        "content": "You definitely mined out a ton of super interesting ideas. The whole that was kind of one of the things about Weaviate that stuck out to me when I first saw it is that you have this like, symbolic vector search. So it's a vector search where you build an index of vector representations, but you also have symbolic traversal. And so, you know, with filtered search, and so and then there's also like the neuro symbolic systems where you parse a scene to get symbolic attributes that you can then put into logic parsing. I love all those ideas. But so I think we're getting now into this like, evolution of I recently did a podcast with Charles Frye that will come out after this one, the title is going to be full stack deep learning. And you know, they have a course called full stack deep learning. And we really dived into like the evolution of, you know, like the MLops and how that's all changing now. And it's kind of like, like this thinking that it's like the perfect minimum viable product tool, the zero shot models and how you can get a sense of it. But then if you do build this data engine to keep training, it offers a potential to have, you know, that unique model behavior that gives you that unique product advantage. So are you thinking about that thing with like, and it seems like Ortus is a great example of that where you can take YouTube videos, put it into whisper, get the trend, like somebody's zero shot models are in the middle of it, right? And put it into a zero shot embedding model that goes to a zero shot LM and then you get like, here's what this thing could look like. Are you thinking about then also having that data engine and what that could then take it to? ",
        "podNumber": 55,
        "clipNumber": 26
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "No, 100%. And, like, obviously, there are some like privacy issues. Currently, we don't even have Google sign in. So there is no way for us to actually couple any personal information with the data. But like, obviously, if you were to just collect the conversations, if you were to collect the likes, dislikes of like, messages, and like summaries, there is plenty of information there and signal we could, we could collect to then, like fine tune, improve models, fine tune them for a particular channel, although there will be, I guess, probably less, maybe for category for a class of podcasts or something, one could probably fine tune them and get some, some defensibility by doing that, as opposed to just just being completely like zero shot, like zero shot is obviously the best way to do an MVP. And the now the problem is, a lot of these closed, closed API, such as OpenAI, like they're, you have to pay premium price for for fine tuning. So I think for for like OpenAI, it's like 6x, like, if you fine tune the model, so that's not economically, really, like, feasible for for many startups. And that's where we can maybe get into the whole MosaicML and all these exciting startups as well. We're training these systems to allow you to have more control, do stuff like constraint sampling guidance from Microsoft, fine tune the model without having to have the 6x, the price, etc, etc. So that's why I'm thinking about this. It's very exciting, super dynamic. ",
        "podNumber": 55,
        "clipNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, the Mosaic, when they announced that 30 billion parameter open source model, and then, you know, seeing the infrastructure they built around the training cloud, you know, because I read the blog post where they're saying basically, it's between $700 and $800 to do a billion tokens of fine tuning on your instruction tuning. And so, you know, I imagine, I don't know how many a billion tokens is, I don't have a great sense of how many that is. But I guess I would have to guess that like my YouTube channel, like the Weaviate YouTube channel transcripts are under that, you know, I'm guessing maybe, you know, all the transcripts and all the tokens you've been processing. ",
        "podNumber": 55,
        "clipNumber": 28
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Yeah, that's a good question. I know that for training, they were like around 1 trillion, right? At least for the 7b model, I know they were around 1 trillion for fine tuning. I don't know on top of my head, how much they use, but I did saw that it's sub $1000 in price. So probably like I think even more than just Weaviate. Weaviate probably doesn't have that you don't you guys don't have that many videos yet. But you're working on it, yeah. ",
        "podNumber": 55,
        "clipNumber": 29
    },
    {
        "speaker": "Connor Shorten",
        "content": "Lets make this a long conversation. But yeah, it makes me think like, you know, having a fine tuned checkpoint from MPT for each of these YouTube videos built into Ortus would be maybe, maybe that's an idea for how this chatbot gets even more powerful in this kind of system. Like, you know, because the I have to, I have to think that if you fine tune the MPT 30 billion checkpoint on Chelsea Finn's lectures of the multitask and meta learning course, I have to think that that LLM performs better than the zero shot open AI LLM. ",
        "podNumber": 55,
        "clipNumber": 30
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Are you sure? That's the part where I'm like, really not not sure. Because if you take a look at the leaderboard from Hugging Face, what's the name? The open LLM leaderboard or something? I think the best model currently there is, well, LLaMa 65B and Falcon 40B. And those are so much better, at least across those couple of benchmarks that they're using for testing there, than the MPT 30B. And so there is a gap there. And there is additional gap between obviously between the biggest LLM and what open AI is offering to GPT-4 especially. And so I don't know, like there is this gap, which open source models are kind of closing, but then open AI is running away. And then one would have to test and fine tune and see how much actually you get, how closer you get to what open AI is doing out of the box. And then is it worth it in the sense, what if they come out with like GPT-5, which is not going to happen that soon. But like, I don't know, you know what I mean? It's ultimately a trade-off. You do have to spend those $700 for fine tuning for a channel or several channels and then keep on doing that as the new episodes are coming out as well. And hope that there is no zero-shot model that can just outperform you. Because ultimately, that's what we're doing, right? Like, it's not unimaginable that if you have like a generally enough system that you don't really have, especially for these videos, it's not that complex. It's not like you're talking nuclear physics or something like, so in that sense, maybe you don't need to even fine tune them if you have powerful enough general model. I don't know. Just a guess. ",
        "podNumber": 55,
        "clipNumber": 31
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. No, that's a great insight. I've heard that argument before as well. It's not worth the investment for fine tuning because yeah, the next generation of the zero-shot model just makes all that money and data collection effort just totally not worth it at all. It's a race. And that's actually what brings my inspiration into thinking that fine tuning embedding models is quite a powerful thing to do because I find it unlikely that, without saying OpenAI, Cohere or a particular company, I find it unlikely that the big zero-shot model off the shelf is going to be able to just do better at your model at embeddings particularly. So yeah, have you thought about training embedding models with all the stuff you're doing? With the Huberman transcripts, because I remember when you first did the Huberman transcripts, you annotated it as MLOps series and that made me think that maybe you were thinking in that kind of headspace. ",
        "podNumber": 55,
        "clipNumber": 32
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Let me think. I think currently the bottleneck for my system is probably less, has less to do with the quality of, if I give a certain paragraph and I'm looking for a neighboring similar paragraphs, I think that Weaviate in combination with some of these models is doing a fair, fairly decent job. I think there are probably easier things to attack, at least when it comes to these types of applications, than tuning embedding model. That's my thinking. For example, the chunking logic, where do you, if you just chunk the sentence in the middle, like you kind of break down the semantics of that whole sentence, that's probably going to hurt you more than the fact that you maybe are not using the best possible embedding model in the world. And so because of all of that, if you were to just sort the bottlenecks, I'm not sure this one will be on top of the list. Although you probably know more than me. I haven't been following the research. I don't know the leaderboard, what are the best ones. I know that Ada002 from OpenAI is very good and they were the best one. That was the best model when it came out, when like five months ago or something, I don't remember. But I'm not sure how the space evolved. Maybe you can tell me a bit more. ",
        "podNumber": 55,
        "clipNumber": 33
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I'd recommend, there's a paper called Benchmarking Commercial Embedding APIs. It's Professor Jimmy Lin. I don't know the exact title, but yeah, so the datasets are like BEIR. BEIR is like, I think it's like 14 open datasets, like financial questions, nutrition facts, MS Marco, and then there's MIRACL, which is like a multilingual, same kind of concept. So they're definitely, I think even with the state of LLMs, the benchmarking and saying, OpenAI has defeated everyone, Cohere, like whoever's defeated everyone, there's not really a great sense of that yet, I don't think, even in embedding. So embeddings would be a tough one to do as well, because to build the embedding benchmarks, you're talking about a crazy data collection effort, annotating the query relevance for a document saying, no, it's only relevant to these documents and so on. So theres like TREC if people are interested in learning more about how people annotate these datasets. And yeah, all that is really interesting, but I want to, the text chunking idea, this is definitely something, that's something we see all the time. It's one of the most common, I think, first steps. Do you see any, like, what do you think about this idea where you use an LLM to parse a document and say, here's a split for a chunk, and you give it some kind of prompt about like, hey, you're splitting this, you know, you're splitting up this document to preserve semantics, please, like, give us this special token when you want to split it. Do you think that idea is worth maybe the cost of all those LLM inferences? ",
        "podNumber": 55,
        "clipNumber": 34
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "That's an interesting question. To be honest, I never thought about it, because the expense and the simple heuristics I'm using it, seem to be, like, doing a decent job. I guess that's similar, that's similar in a way to the thing I mentioned before with the saliency when you were trying to chunk the videos, I extract certain frames that you want to ultimately feed to this multimodal model. So here, we're kind of trying to understand where you want to chunk these, so that you can then use whatever embedding logic or whatever you want to do. I'm not sure, do you have, like, are there any papers doing this already? Like, I haven't been following that closely. ",
        "podNumber": 55,
        "clipNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, well, on kind of like a meta note, since I've joined Weaviate, I don't really read papers as much to get this, my information, like, my information diet is really not papers as much anymore. It's more like, you know, just like, I'll have a conversation with someone who's building something with Weaviate that's like, about as random as you can imagine. And I have tons of calls like this. And so I, and this is kind of where I pick up these nuggets is just like, where people are kind of trying out there in a more of a hacking kind of sense than like, something that I'm going to say, like, this paper from Stanford and MIT, they tested it as rigorously as possible, right. But I guess, like another kind of cool idea to float out there about text chunking is people are thinking about like putting metadata into the chunks. So like, you would, you'd find the title, and then you'd keep putting the title in there. There's maybe some stuff too, with like visual document layout parsing. But I don't know if it generalizes that well to YouTube, unless you're like, I guess what you're explaining to me in this podcast that I hadn't, I hadn't understood about Ortus previously, is that you're trying to do something with the visual aspect of YouTube as well. So, and so... ",
        "podNumber": 55,
        "clipNumber": 36
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Well, ultimately, like my goal is to, to like provide this type of like insight and like extract information in O(1) instead of you having to, like historically, you'd have to go through the thumbnails and see where was the thing that happened and then try to kind of, you're using a bunch of heuristics to find information. Like, obviously, for some type of content, for this class of content, where you have more, like sedentary, like people sitting down in a podcast and just like chatting with the two of us right now, probably like it's, it's probably like the visual component does not add that much other than maybe emotion, interpreting, like whatever, like in that sense, but most people don't care about it, right? Like you just, you're kind of invariant to the, you're just like focusing on the knowledge and you try and ignore like the perks of each person's like, like behavior or whatnot. Like, so, so I'm not sure it would add that much to these videos, but like in general, it's a no brainer that for some, maybe action, more action driven videos, shorter videos, like sometimes the visual component all of a sudden becomes much more important than textual component, right? Especially when you don't have that, when you don't have voice. So, and I'm definitely thinking about this and I used to work on vision language models at DeepMind, like I was closely working with Flamingo folks and I was actually helping bring Flamingos to the real world. I can't really dig into too much details, but like I'm very, definitely super passionate about this space. And I was even before I joined and maybe getting back to the, to the initial question you had, I think that's kind of my niche. Like I found this combination of language and, and, and, and like imagery or videos and audio, like the multimodal space, the most exciting, like research direction, at least over the next couple of years, I think it's exciting.",
        "podNumber": 55,
        "clipNumber": 37
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. I think like maybe to tell a quick story, like before I was doing deep learning, I was playing basketball and I used to love doing like these, like you try to like dunk as well as you can. And there are like trick dunks you can do, like you could throw it off the backboard and dunk it. You could do like a windmill dunk, you could do like a 360 dunk. And so you, and like this, I, if people up there, like YouTube channels called like dunkademics and stuff like that, where people search for similar dunks and I can, like, like this idea of searching for similar dunks would be quite a novel idea. And so I think there's certainly like a huge thing, I guess, like I'm so biased in thinking about this kind of like information heavy stuff, which brings me back into it. ",
        "podNumber": 55,
        "clipNumber": 38
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Yeah. And you work at Weaviate, so you're, you're kind of, yeah, definitely, definitely biased in that respect. But like, yeah, there is exciting stuff to be done in the, in the visual space. I think one company that's doing a tremendous work, although they're like Gong, you might have heard of them. They're, they're in the sales vertical. And what they're doing is they're helping understand what their, what your sales represent, representatives in your company, how confident are they, like brand reputation, all of this information, they're, they're kind of like extracting the data using AI, I guess, in the background, like they're probably using some heuristics. And maybe, maybe since recently, they started using more neural networks, if at all. But like, that's one clear example where, where like people are using like visual component in industry to analyze and extract information from the radio. ",
        "podNumber": 55,
        "clipNumber": 39
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think I know, we're talking all sorts of topics. But yeah, like CRMs, I think are, are, like, just how you process a call and all that is just, and it kind of brings me into like, what, like what I love, my, my, like, favorite project to work on, which I feel like is kind of adjacent to Ortus is like, I turn this, these podcasts into a data set, in the Weaviate podcast search on GitHub. And like, I'm so excited about thinking about what, what can be done with these podcast transcriptions.",
        "podNumber": 55,
        "clipNumber": 40
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "And I want a compensation for the data provided. ",
        "podNumber": 55,
        "clipNumber": 41
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I'll pay you in two github stars. However I'll transfer the currency. But yeah, the, the, there's one, I there's like these two ideas I so I had Yuxiang Wu on one of the podcasts who was working on something called chat arena, which is where you like simulate conversations. Do you think there could be anything to that? I know we're talking like, just totally broadly, like about whatever now.",
        "podNumber": 55,
        "clipNumber": 42
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Simulating conversations with like, with the idea of like generating more data, or, or with the idea of entertainment or like, or just Yeah. Yes. So it sounds feasible. But why? There could be many wise. Yeah. Right now we're calling this generative feedback loops to like broadly describe this idea of like, the feedback loop is in reference to saving the generated data back into the database to then search through it in the future. And so the idea would be you simulate these conversations, and then you like save the new conversations back and you blow up the this data space this way. And you know, like, I guess the idea being that in simulating like a million conversations between like Sam Altman and David Silver, you know, just like taking people to just chat about these topics with that you create like this new space and because I kind of think like, you know, it reminds me back to like early days of training GANs where I'd be like, what kind of novel images has it produced? And so I would need to generate those images. And that, you know, this is way before I knew about something like a vector database or even thought about that kind of idea. But you would just like, it was kind of a natural intuition to try to like, sample models from the GAN and then embed them and, you know, compare the nearest neighbors from your GAN to your training set and just say like, especially like when youre debugging mode collapse. Hopefully that sets the stage enough. ",
        "podNumber": 55,
        "clipNumber": 43
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Yeah, yeah, yeah. It makes sense. Well, yeah, like, I guess this space of conversations and language might be even much tougher, right? To find the nearest neighbor of a conversation rather than of like, 256 by 256 image or 1000 by 1000, whatever with GANs, they did improve. But yeah, definitely interesting, like research idea. And you can definitely imagine products coming out of that direction immediately. Like, I'm sure people would be willing to pay to hear, like, I don't know, historical figures like Nikola Tesla speaking with Albert Einstein, or you know what I mean? Like, so something similar in spirit to what Character AI is doing, but more multi-agent, more like more of that. That could be one interesting product area. And the research is also interesting in itself. ",
        "podNumber": 55,
        "clipNumber": 44
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I guess it's like, by simulating conversations, can you discover new, like, ideas? Kind of like, maybe like, another kind of thing I like a lot about is like, to take these podcast transcriptions, you know, automatically take out the chapters and maybe prompt it by saying, like, you know, Aleksa and Connor are talking about vision and language models, like, retrieve from all the other podcasts and say, did they say anything unique? Is there anything like new in this conversation? Prompts like that. And then you kind of from there, the things that are new, I don't know, like how many new ideas you discover in conversation. ",
        "podNumber": 55,
        "clipNumber": 45
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "We might get depressed, man. We might get depressed. I think we're giving ourselves much more credit than, than, than, yeah. We're just interpolating, man. We're just interpolating. Even though we are, everybody's bashing neural networks. So they're just interpolating data and like humans. Like if you think carefully enough, like you just, you just find a snippet of information you picked up five years ago, and then you connected it with some snippet information you found out two days ago. And all of a sudden, this, this, this seems novel and interesting, but yeah. Existential. ",
        "podNumber": 55,
        "clipNumber": 46
    },
    {
        "speaker": "Connor Shorten",
        "content": "Like this kind of synthetic data, if I can, I, so I saw this interview with Aiden Gomez, CEO of Cohere, talking about synthetic data. And I sampled this quote out where he says, I think there's a way in which synthetic data leads to the exact opposite of model collapse, like information and knowledge discovery, expanding a model's knowledge beyond what it was shown in its human data. That feels like the next frontier, the next unlock for another very steep increase in model performance, getting these models to be able to self-improve, to expand their knowledge by themselves without a human having to teach them that knowledge. So that's like the end of the quote I read. But I would like the, I like this a lot because I think whether you're simulating conversations or you're putting them in an environment to simulate, and this is kind of a deep mind-y idea, right? Like this open-endedness, I think they have these, it's like this kind of synthetic data, like, I guess, I think that quote maybe does a better job of explaining that than I do, but like having these conversations is like creating synthetic data and maybe there's new ideas in there. ",
        "podNumber": 55,
        "clipNumber": 47
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "100%. And ultimately, if you think about it, when you're, when you're trying to solve a problem, I don't know about you, but oftentimes I do sub-vocalize. I do talk with myself, like I do have inner characters, like communicating with each other, right? And trying to, to get to the conclusion or, or when you're trying to debate out an idea in your head, right? You're rolling out the discussion, like a hypothetical future. How would that, what would be like, like if you had an adversarial or maybe a better word would be like a super smart, like opponent and, and like how, how would you reply to all their questions and where would you, like, how would you traverse that whole like tree of potential outcomes? And so like, in that sense, I think this is also more fundamental than, than just like data or, or like it might be a methodology to improve the reasoning, like capabilities of these systems. It's definitely super fun. Super interesting. ",
        "podNumber": 55,
        "clipNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. I think like also staying on the deep, like, like the mute, the alpha go, how it did this tree search to look into the future of the moves it can make. And I don't know if you've seen this tree of thoughts paper where it's like a language model is like, yeah. Like, so yeah. What do you think that tree of thoughts is? I mean, to me, that seems like one of the most craziest ideas I've ever heard is combining the. ",
        "podNumber": 55,
        "clipNumber": 49
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Yeah, definitely. Definitely. Super exciting. It's kind of the generalization of the, of the chain of thoughts, but just going from a linear process to a more like tree-like process in that sense, conceptually, it's, it's not that like, well, I don't want to say big of a deal because all of these ideas in retrospect are just connecting a couple of dots. The problem, I actually wanted to try out that idea in, in, in, in our system in Ortus. But the thing is that that's super expensive. That, that type of like tree, like you, you, you, you need so many inference steps and that's more for some offline processing of some reasoning tasks. But, but definitely interesting. ",
        "podNumber": 55,
        "clipNumber": 50
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. Yeah. Well, that's, I mean, I think the cost of inference with large language models is so frustrating because it feels like there's so much we could do if it costs nearly nothing to just run. Yeah. ",
        "podNumber": 55,
        "clipNumber": 51
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Yeah. But it's also liberating because it constrains a lot of the potential methods that could be trying, right? Like when you have the real time constraint, when you're interacting with the users, when there is the latency component in the whole picture, then you start thinking in a completely different direction. And you realize that there is not that many great ideas that focus on like low latency, real time type of, of, of like applications. It's mostly, you're trying to get the model to be better and then, and distill it and make it, make it faster than, than all of these techniques. There are more for these mathematical logical reasoning benchmarks than for like communication and stuff. I don't know. ",
        "podNumber": 55,
        "clipNumber": 52
    },
    {
        "speaker": "Connor Shorten",
        "content": "Like, yeah, like kind of like the tool use and the, well, yeah. And the whole distillation. Yeah. And all of these clever things. I mean, it's like, how exactly are they going to compress these models and get to the point where it's super cheap to run it? Like I've seen like this, you know, like four bit inferences getting really good. And like the stuff that Tim Dettmers does is always like, you always see that tweet and you're like, why? That's just insane. Like, you know, I think it's like, you know, they're at like 60 billion on a single GPU, stuff like this. But yeah, so yeah, this whole kind of topic. And so I think kind of, I think we've done a nice, like just open-ended touring of topics around Ortus. Is there anything maybe before moving on, I wanted to ask you kind of about some personal or personal self-organization things I took away from your podcast with Ken Jee recently. But is there anything maybe you want to cover as well about Ortus before we kind of move on topics?",
        "podNumber": 55,
        "clipNumber": 53
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Yeah, I encourage people to try it out! Self-plug! We just added a couple new features, we added suggested questions for each video there are a couple of relevant suggested questions because there is this blank page syndrome, I think thats what its called. That is what writers usually have but also when you are interacting with these systems, people dont know what to ask. Even though, you can actually ask the agent and it can tell you what it can do, but people dont com eup with the idea. So this small UI effect so to speak helps a lot I think. We also added Jeremy Howard today, there is a bunch of new channels coming out! Im learning a ton, also learning a ton about the whole backend business. For example, I spent the whole day today trying to figure out the google sign-in. Even though its super conceptually easy, it turns out its very difficult because of versioning hell, the manifesting chrome version 2 version 3. A lot of the documentation is outdated. People dont give engineers enough credit, in this ML space we focus too much on these interesting research ideas, but getting shit done is so much more difficult sometimes than you would like to think, even when its conceptually super easy.",
        "podNumber": 55,
        "clipNumber": 54
    },
    {
        "speaker": "Connor Shorten",
        "content": "The question I really wanted to ask you, hopefully the connection loss didnt lose what you said about adding Jeremy Howards channel into Ortus. And as Ive been following along with you, youve been adding all these new channels into it and its a nice continual updating of it. I am really wondering about this idea of an embedding marketplace and its kind of like an interesting new category, Ive been having a few conversations with entrepreneurs who are exploring this idea, say you want the Almanack of Naval Ravikant, if you want the OpenAI embeddings of that book, you can just grab it from this marketplace. I think that makes a lot of sense because you dont have to think about vectorizing the thing because its already been done for you. Do you think that could be an aspect of Ortus as well?",
        "podNumber": 55,
        "clipNumber": 55
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Sounds exciting, I never thought about it because I guess it always looked in my mind as a very custom thing, right? You have a lot of hyperparameters you have to pick to do that type of embedding marketplace as you called it. How big are going to be the paragraphs, sometimes you want to have smaller ones, sometimes you want to have bigger ones, where do you chunk? Chunking decisions matter. People figure out better methods, like the LLM saliency method you mentioned and then there is also the choice of the embedding model, and then we were discussing also the quality of embedding models. So what happens if all of a sudden a much better embedding model comes out, what do you? You have to re-index the whole marketplace? So in that sense, Im just thinking why didnt we ever do the same thing with other types of datasets, like for example with ImageNet. Why didnt we create a marketplace of the embeddings of certain layer inside of ResNet-50, that was such a common task right, people were constantly using ResNets back in the day and you always had to do this preprocessing to save on compute and then you would locally deal with the data. So why didnt anybody come up with that type of marketplace for other types of models, not just embedding ones. I guess thats one question, I dont know how you think about.",
        "podNumber": 55,
        "clipNumber": 56
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think thats perfect. The way that I understand ImageNet, maybe this is a hot take. They put a ton of effort into labeling the dataset and its kind of like you need to like kind of apply for it, its not just something you can just download on TensorFlow datasets. And I feel like they kind of gatekeep, paywall, not paywall but you know, But its kind of like the MIMIC dataset, you have to get this certificate from PhysioNet and then you have to go and apply for the dataset. So thats kind of my opinion of ImageNet.",
        "podNumber": 55,
        "clipNumber": 57
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "More gatekeeping mentality. Thats a fairly normal thing in AI, I dont think youre stepping on anyones toes. I think thats a shared understanding.",
        "podNumber": 55,
        "clipNumber": 58
    },
    {
        "speaker": "Connor Shorten",
        "content": "So yeah, but I think with these YouTube transcript datasets. Its like if you open-source a dataset then scientists, can you could publish a paper where you try different text chunking with some kind of metric. Maybe you generate synthetic queries for Jeremy Howard videos and then thats how you measure it. And then you can publish science and present it in NeurIPS, ICML, and stuff like that. And so, I guess its kind of interesting to me especially like with Weaviate development and its like what kind of dataset do we want to, like with ANN benchmarks, we make some innovation on Product Quantization, ok were going to tell you about the SIFT vectors recall, and thats pretty academically sound, but I worry if that tells the story. Compared to the dataset Ortus, if we could show some innovation in embeddings that creates this better experience for Ortus by using these transcripts as the dataset, and then also get like the academic credit of like we did this clever technique because were geniuses.",
        "podNumber": 55,
        "clipNumber": 59
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Im definitely not in that game. I should gone the Ph.D. route, but I took a completely different direction.",
        "podNumber": 55,
        "clipNumber": 60
    },
    {
        "speaker": "Connor Shorten",
        "content": "So anyways, yeah I think its quite an interesting idea and I think theres all sorts of cool things you can do with Ortus. When I first saw your Andrew Huberman transcripts podcast demo with the website, I thought that was such a cool demo. I encourage everyone to check out Ortus of course. So transitioning topics a bit, I think you have a lot of interesting ideas about personal organization. Obviously, youve reached the height of Machine Learning success at DeepMind and now youre a startup CEO / Founder and so you obviously need to think about this quite a lot. One idea you mentioned in Kens podcast is analogs of periodization in weight lifting and how you think about your information diet. Perioidization maybe you want to explain the idea.",
        "podNumber": 55,
        "clipNumber": 61
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "The idea is fairly simple. When you are trying to build muscles in powerlifting, you are not going to try and increase your weights every single workout.Because ultimately you are going to saturate and probably you are going to hurt yourself. So what they have is you keep on increasing for a microcycle of 3 month periods and you keep increasing the weigihts every 2 workouts you increase maybe 2 kilos. And then you drop by some percentage. If your max rep is 300 kilos, your next workout is maybe 80% and then 60% and you start from there. You do this this so called deloading and start a new cycle. So I borrowed this terminology when I was learning new subjects because I treat brain like a physical thing as it is. Ultimately you have to form these synapses and it is a biological process and memory has to move around. So because of that I think its reasonable to make these kind of breaks and periods and organize learning in this way. I think its a fairly simple analogy, I would be surprised if nobody has come up with this well.",
        "podNumber": 55,
        "clipNumber": 62
    },
    {
        "speaker": "Connor Shorten",
        "content": "When I was listening to you speak about it recently I thought it was so interesting because in weightlifting, I think rep ranges is the thing to think about. You have like a hypertrophy cycle, you know where you do 2-4, 4-8, 8-12 and it ends up being an entirely different workout. And I think the analog here is whether you are reading papers, hacking a project, writing deeply, or if say you are interviewing someone and talking to customers of your product. So these are like 4 separate talks.",
        "podNumber": 55,
        "clipNumber": 63
    },
    {
        "speaker": "Connor Shorten",
        "content": "So do you think about organizing your time that way. For me, its honestly a chaotic mixup of these, there is no order to what its done. I dont have like a reading week, engineering week, talking to people week.",
        "podNumber": 55,
        "clipNumber": 65
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Currently its more on a day to day basis. It depends on the stage of your life, back then I was trying to learn so many topics. Currently I do have some structure but its more on a daily basis, I know when I have a workout. For example, I just ahd a workout immediately before this podcast. I have some nutrition after the workout. I have regular sleeping hours and I have maybe cold shower around 5PM. So I do have some structure and around the structure I try to put in everything else. If I dont have it I feel Im not as productive. I feel its very important to have some kind of rigidity to constrain your time one way or another.",
        "podNumber": 55,
        "clipNumber": 66
    },
    {
        "speaker": "Connor Shorten",
        "content": "I think something thats worked for me, like little things. I try to have meetings everyday in two time slots between 11-1 and 4-6, if you know, it allows itself to be organized that way. And then I like to have those in the calendar and then I have my morning and afternoon session. And then the pressure of if I dont do my deep work before the 11-1 then its lost. So these are little things I do, I maybe dont do cold showers as often as I should.",
        "podNumber": 55,
        "clipNumber": 67
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Again I think, Paul Graham had this distinction between the builders, the makers and then was it the managers? I think Im missing out on this other group because Im not a part of that group. When you are a maker or builder you need these long periods of time with no interruptions. And so if you do have a chance, I can just not have meetings at all. I rarely have meetings to be honest, I recently started attending podcasts on Fridays. But most of my time I try to get into this flow. To get into this flow, you do need spans of time to just get deeply into something, otherwise I dont feel as productive.",
        "podNumber": 55,
        "clipNumber": 68
    },
    {
        "speaker": "Connor Shorten",
        "content": "Its so interesting. I also agree, for me its like more than 3 hours Ill probably still be on my computer thats why I like to do 3 hours at a time and then meetings. The other kind of meta topic is as youre building Ortus how has your perspective changed on Breadth-first and Depth-first thinking. You mention debugging this Google Sign-In task, how would you categorize that kind of task?",
        "podNumber": 55,
        "clipNumber": 69
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "I would categorize it as breadth-first still, because its one day. I do a bunch of stuff, I am currently a single employee and my girlfriend is doing product design and she is helping with other stuff. But like the technical component, Im basically building the whole system I have to do the whole data pipeline, the roubstness of the data pipeline, the backend, the SLA, the front end part, and thinking about UI, and having user studies. I actually started having more user studies, and some decisions I made were a consequence of those user studies. So its fairly broad, there is the marketing component obviously. The community outreach, all of that. So its very very all over the place. Its not focused in the same sense as some PhD guy or someone working at DeepMind. If you take a look at this day as a time unit, I was just doing this obsessively for like 6-7 hours or something.",
        "podNumber": 55,
        "clipNumber": 70
    },
    {
        "speaker": "Connor Shorten",
        "content": "My personality, maybe you share this, is Im doing 6-7 hours of thinking about like a gradient spiking error and debugging it for all that time. I dont really have the attention to do that, I need to have a little bit of marketing. To me the breadth-first helps me to keep having fun.",
        "podNumber": 55,
        "clipNumber": 71
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "I definitely can emphasize with you, but still I can pull it through. Oftentimes there will be issues where you need to push through and do just a single thing for days. I think I do have that skill. It is harder, there is like a physical manifestation of pain, because oh my god, you are just pissed off with everything. But you have to push it through, and so its a skill it can be built as anything else. But definitely more enjoyable when you have this broader understanding of it.",
        "podNumber": 55,
        "clipNumber": 72
    },
    {
        "speaker": "Connor Shorten",
        "content": "I think the psychology of the most painful thing you can do. I think the most painful thing is when you are solving a coding error, when you do solve it, its not intellectually satisfying at all. Its just like I cant believe that was the thing.",
        "podNumber": 55,
        "clipNumber": 73
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "But I think when youre an engineer, a software engineer. You learn to appreciate those moments because most of your job looks like that. I really like Greg Brockman, at least according to what I can see, his tweets, he seems to be that kind of person. He is pushing through these very painful, boring, nitty-gritty details. There was some off by one error, or redundant copying between cpu and gpu host. After 7 hours of digging deeply, he found the problem, it was obvious in retrospect. But until you find it, the asymmetry between verifying versus finding the solution, very exciting.",
        "podNumber": 55,
        "clipNumber": 74
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, there are a lot of people at Weaviate who do that better than I do and I just respect it enormously. It looks hard, every now and then I find myself doing problems like that. But that just getting into like the cpu profiling and all that kind of stuff its really a huge skill. Another thing you said that is really interesting and I relate a lot in my life is, my fiance also works at Weaviate and we work together as a couple that also talks a lot about work. So Im really interested to hear that your girlfriend is doing the product design, my experience has been its just a ton of fun and maybe we talk about work maybe more often than a normal couple would I would say. But it also ends up being an nice conversation topic in your relationship.",
        "podNumber": 55,
        "clipNumber": 75
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "I dont know how my relationship would look like if we werent connected on this project. Most of my time, since the moment I wake up until 2 or 3 am I am constantly working in one way or another. With a couple of these breaks I mentioned, because they are important. So like Im not sure where I could squeeze in that additional time to keep the relationship healthy. So Im grateful we are super aligned and we are both aware of that dynamic. I think thats exciting, maybe a thing that popped into my mind, if you know the guy who is the CEO of Replit, Ahmjad, his wife is his co-founder. They are a unicorn company and they have been in the game for like 7 years. I really love to hear those stories where people are not inside of the common pattern, I really respect that, because the pattern is you want to be 2-3 founders, mostly guys, and then like preferably Stanford, etc. etc. so you know YC has this like pattern matching and so when you hear such an outlier succeeds, its very motivation as well, because it means maybe now we can do this. Look at me, I came from Serbia, just self-studying, and then I am in the top lab in the world, and maybe I can do this. And so I think these are very important just for like hope in the world I guess.",
        "podNumber": 55,
        "clipNumber": 76
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome, well Aleksa man, thank you so much for joining the podcast I had so much fun talking about all the technical ideas as well as how you are managing your time. Ortus is such an exciting project, Im so excited to see how it evolves. Thank you so much again.",
        "podNumber": 55,
        "clipNumber": 77
    },
    {
        "speaker": "Aleksa Gordcic",
        "content": "Thanks a lot man, I really enjoyed the conversation.",
        "podNumber": 55,
        "clipNumber": 78
    }
]