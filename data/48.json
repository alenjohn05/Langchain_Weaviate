[
    {
        "speaker": "Connor Shorten",
        "content": "Hey, everyone, thank you so much for watching the Weaviate podcast. I'm super excited to welcome Brian Raymond, the founder and CEO of Unstructured. Unstructured is such an exciting company for, I'll explain kind of the way that I see the space of kind of how do you get unstructured data, like say PDFs or these corporate data lakes, web pages, all this stuff, kind of getting it into systems like Weaviate. And so I'm so excited to welcome Brian, thank you so much for joining the podcast. ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Thanks for having me excited to be here.",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. So could you maybe kick it off? I know I'm going to give an of course description, but can you explain what unstructured is and the founding vision of it? ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Yeah, absolutely. So when we we embarked on this kind of journey that we're on today with unstructured, the vision was pre processing is terrible. Nobody likes it. It takes forever. But it's a critical step if you actually want to deploy language models of any type against data that's important to you. And so I had the last four years before I started unstructured, I was with a great company called Primer AI. And at Primer, we were fine tuning models, orchestrating pipelines, building applications on top of them doing all sorts of really cool stuff with transformer based models. And, and we we'd have these, you know, fantastic applications, and we'd show them to customers and say, I want that on my data. And then we'd look at their data and we'd hold their head in our hands. This is gonna be amount of work and regex and Python scripts and OCR and everything just to get everything into typically like a nice clean JSON format. Right so that we can feed it into an inference pipeline. And so the vision when we started out was, hey, HuggingFace is, you know, exploding over here with, you know, 10s of 1000s of model, incredible community. What if we did something similar to the left of HuggingFace, and we made it cheap, fast and easy for data scientists to get through that data engineering step, so they can consume more of that. And so, you know, the, you know, initially, it was, hey, let's help folks build out these bespoke pre processing pipelines to take, say, a non disclosure agreement that's in a PDF, you know, and train classifier model or something on it, and you get that into JSON, or in a bunch of PowerPoints or CRM data or scraped HTML, how do you get that to a point where you can start interacting with it. And, and so, for the past 10 months or so, we've just been been in build mode, you know, focused like a laser on ingestion and pre processing a little bit of work on connectors, but mainly on that transformation, that file transformation work to get it into a base format that you can go use with machine learning.",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Absolutely fascinating. Really, you know, captivating overview of the thing I to kind of maybe to break that into pre processing is this like, are you thinking about like in the I know, obviously, there's like the PDFs to Weaviate a kind of angle that we're so excited about learning more about, but I think also you're hitting on like, kind of like the ETL, like when you are data scientists, and you take like a CSV, and you look for like missing values, the range of the distribution of your features, is this the kind of thing that you think about like, that kind of pre processing? Can you tell me more about the pre processing?",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Yeah, and it's its own weird space here on pre processing with files that contain natural language data. And so we think about it in in internally in terms of like three steps, okay, the first thing you want to do is you want to partition and extract that the natural language data from a particular file, right? And so if it's, you know, a JPEG that\u2019s of a billboard, or if it's a word document, or a text file, or whatever, you still want to detect, you know, you want to detect like, okay, this is the headline, this is body text, this is an image caption, because you want to be thoughtful about down the down the stream about what you put into your inference pipeline. You want to extract that data, and that's a whole process in and of itself of doing that. And so, once you've done that, once you've partitioned and extracted that data, the next step in our world is cleaning, and how do you get reduce it down to like a clean markdown file or JSON. So there's all sorts of artifacts that that that, you know, wind their way in here. So like, sentence fragments, and what weird white spaces and characters and all sorts of just like little gremlins that just burn up time on on cleaning that up. And so we spent a lot of time thinking about that cleaning step. And so once you have that clean step, as a data scientist, your job's still not you're not ready yet, you got to stage it. And there's a lot of work that is in the staging still, which is last mile that burns up time. So concatenation, chunking, tokenization, creating embeddings, mapping JSON schemas for, you know, maybe you want to set up a labeling task, maybe you want to dump it in vector DB, maybe you want to send it to a hugging face pipeline, whatever that might be. There's a lot of that last mile kind of staging work that has to be done. So partition, clean, stage, and then you're ready for ml. And all of that today is still completely manual. And that's why 80% of data scientists work in this domain, actually on this pre processing stuff that they don't want to be doing, they want to be doing the modeling. And so we're trying to give that time back to them. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, amazing. Data scientists always saying like, all the data cleaning, the most painful part, but the most productive part and see the prop, the pain point is so clear, and the problem being solved, you know, so useful. Um, so I think kind of I want to take into this question of the why now question sort of like the how large language models are facilitating unstructured in this technology, what what's what's now making it easier? ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "I think what large language models have done is really increased the demand side of the equation, the right hand side of the equation, where there's a there's a couple attributes of them that are really important. One of them is that from a generation, I'm not telling anyone anything they don't already know, but like, they're just a lot easier to use for generative tasks, right, then, then all the kind of the older class of models. And so it's just the time to value and the cost of eyes a lot lower. And the second is that there are a lot less fragile as brittle to to the data that you're putting in them and to the problem. If you're using like in context, learning or whatever. And so what that has what that means for us is that our persona has shifted a little bit. So we were in a world where you needed to create these incredibly accurate pre processing pipelines, because if there's any noise or shift in the document template or anything, it would throw off a lot of the the downstream, the structured data that may be fed into a knowledge graph or something downstream. Now, um, you can there since they're a bit more tolerant, and there's a lot more that you can do with them. Um, folks are saying, Hey, I want to use everything that we're producing, like every file type everywhere we're creating. We're recreating, you know, natural language data that's relevant to our organization. I want to I want to use it in conjunction with a language model, right? And so on our end, it's gone from lots of super precise pre processing pipelines to, hey, how do I take an s3 bucket or an Azure blob full of like, you know, just everything that you can imagine, send it through Unstructured, and then dump it in Weaviate. And now I can chat my data or interact with it, right. And so that's shifted some of our, like the paradigm that we're approaching a lot of our engineering work and what we're prioritizing. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Gotcha, gotcha. So let me get your thoughts on that was, I remember like with LangChain and llama index, when they came out, they had like a data connectors hub. And I remember things like s3, Google Drive, notion, you know, like, or just like the PDFs. So what was your reaction to that kind of thing? Is that that is that kind of the, like a lot of the Unstructured part is like connecting to these data sources, as well as the transformations? ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Oh, yeah, I mean, Jerry and Harrison have done just awesome work there. And, and unlocking a lot of this, the potential over here, we're looking to kind of build on their shoulders for enterprise production grade deployments. And it's just a slightly different sort of take on it. That's feeding into our engineering requirements. And so what that means in practical terms is that like, we're, I'll talk about our connectors and how we're thinking about connectors versus data loaders, right, and the compliments there. And then also what we're doing on the transform side that's not there. And really, this is a story about how do you enable all the data, like the 1000s of data scientists out there right now, that are prototyping are going to go from prototype to production. And I know Harrison's been spending a ton of time talking about productionization of agents and these other things. We're, we're in this narrow strike, we're staying disciplined in this narrow strike zone of ingestion and pre processing. So on the connector side, some some things that differentiate our abstractions from, from those data loaders, are that the data loaders are kind of grabbing everything and moving them over, we're able to ascribe canonical numbers to them. And so we can measure net new, and so not grab everything. And so we're not like, you're not shifting a lot of duplicate data in. And then also thinking about scheduling, and these will be, these will be supported by us. So if they break in the middle of the night, unstructured will fix kind of like a five trend type approach, but they're built for the ground up from the ground up with LLM is in mind. And so some other things like they were building them in such a way that they lend themselves to parallelization across like CPUs. And so and so that's like a whole discussion in and of itself on the connector side. But once you connect and you grab that data, that net new data from G drive or SharePoint or Azure blob or whatever, we're doing all that transform work that you know, the partition, clean, stage, right and then handing off like our goal is our is for our users to be able to take raw data, grab it, transform it and then hand off wherever it might go might be a vector database like Weaviate without having to worry about any additional data engineering. It's ready to go. And so we've gone from raw to ml ready within Unstructured. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "And so so I think that there's a huge innovation in, I wanted to talk about this kind of like PDFs to we've I think this is one of the most exciting topics is that you know, we've seen this at hackathons and stuff as people they want to chat with their PDFs because it makes a lot of sense. So could we talk about the innovations in Unstructured like how would you process a PDF with unstructured? ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Yeah, we were trying we're doing a ton of internal R&D work right now in this area that spans a few different approaches. And we're what we're imagining in our for our kind of target persona or folks that are like our organizations that are producing huge volumes of data today. So we're thinking a lot about compute efficiency, like some of the large organizations that we're working with today, create and process more than like a quarter million files a day. And so I want to burn up all your compute budget on pre processing, right? And LLM are, even though you can use LLM for some of these things, they're about 100 times more expensive than the approach that we're taking. And so we're like, okay, how do you render the highest quality data with the least amount of compute so that you can get to a chat your data type of approach. And so one of them we're taking like Yolo X, and we're, you know, fine tuning that on our internal labeled data, we got like 10s of 1000s of pieces of labeled data that we've been curating over the last several months. And so that's, you know, Yolo X is creating the bounding boxes, you know, choose your OCR vendor, right, whatever. And then couple it with our staging bricks. And now voila, you're ready to rock and roll. That's one approach. Another approach is like a donut based approach. So a OCR less vision trends like swim, vision transformer, where we've been doing a lot of work there. Some performance, you know, benefits and trade offs between that and an OCR approach, and then also implications for compute. And, and so that's like another one, you'll be seeing those APIs coming out soon, our general pipeline, which is realized more on like, you know, more traditional CV and NLP approaches, and that's available today. And then we're also pre training our own foundation model from the ground up. That's our own vision transformer, where we've curated all the pre training data. And we've curated all the fine tuning data and with the with the idea of being, hey, let's train a model that you can throw almost anything at it, and it's seen it and it can, you know, transform it and render it with a high degree of performance. That's in process to actually are just starting, we're just wrapping up pre training and transitioning to fine tuning on that. And so the goal is by mid summer, to have a wide range of different kind of like arrows in our quiver that our users can can use to, to get over that hump and to get to that clean JSON. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Amazing. A couple things I want to take apart for first day, I think just your knowledge of the state of the art and OCR. And I think Swin transformers, can you just kind of explain that more? Because, you know, I know, I don't know that much about I'm sure our listeners are curious about it. ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Sure. I think I mean, at a high level here, a lot of the approaches to date have been okay, infer bounding boxes around, around document elements. And there's some models that work pretty well on that we, we've been doing a lot of really focused work on on I mean, folks, stuff that'll just put a lot of people to bed to sleep. But how do you actually draw a bounding box around an image caption and differentiate list from a header or a footer or any of these things so that you have accurate metadata tags, you can strip all that stuff out and focus like a laser and exactly what you want downstream so you can curate your data. And so that's, you know, vision, you know, a computer vision model coupled with OCR to do the extraction, and then you know, you can do some last mile tuning before you roll. On the vision transformer based approach. It's really interesting. So it's, you know, it's, it's tiling up a particular image. And, and it's, it's using an OCR less approach with so it's a vision encoder, and then a text decoder order to do the jump from an image in to JSON out. And, and, you know, we were pretty optimistic about that approach, both from a compute standpoint and a performance, but depending on the needs of the user, if the user is going to need like coordinates for traceability, like where a particular extraction came from, and which page and where on a document, you know, those all those specifics may necessitate different, like a different approach. And so our goal is to have like a nice wide menu of approaches that kind of balance features with with compute efficiency. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "It's so interesting to learn about all this. I mean, I, well, I want to come back into the plans of the foundation model. I'm very curious, like how you're seeing training a custom foundation model and exactly what that'll look like for unstructured, but I have quickly I'm curious about like with the so so with visual document layout, and then I'm, I feel like web scraping is a huge application of this where I'm like, you know, we also just record a podcast with the Kapa guys who took all the documentation blog posts of Weaviate and put it into one of these retrieval augments and systems. And I mean, it's really remarkable. And so, so like, you could you just point me to a web, like if I just had weaviate.io, just this website, how would you then go about getting all the data?",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Like, so like, what we're doing is, instead of trying to parse the HTML, like really effectively, and because it can be like, there's nothing, there's some solutions for it that work pretty well, but they're not like, fantastic. So what we're doing is instead, we're, we're reassembling the web pages, and then imaging those pages, and then feeding that those, essentially, like screenshots of the web pages, through the model, right in order to do that. And we think that that approach, and there's been some recent, like, really great Twitter threads and some some papers on this, that the compute efficiency of these computer vision models is coming down. And so you can do that at scale. And the performance of those extractions is going to be a lot better than what you can do in just the pure HTML parsing. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that that is incredibly exciting. I mean, I think, yeah, wow, it's really like you're really painting a compelling vision for the future of multimodal. I think about just like, you know, I like I like to read a lot of scientific papers. And I think about how much information is captured visually in the diagrams. But then the next question, and this is something that comes that I've heard so many times with the vector searches, like, so with tables, you can also kind of like extract tables and, you know, charts and stuff. So is so yeah, I mean, I don't know if it's the question is maybe pretty self obvious. I think you do the same kind of thing as the OCR models and all that. So yeah, so I think it would be a good transition to come back to that foundation model. So training your own foundation model to do this kind of, I want to say, document layout parsing. Is that kind of the idea? ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Yeah, yeah, raw to JSON for files that contain natural language. And so feed in anything, almost any file. And what the approach that we're taking is converted to with this particular model of other options, but with this particular model, convert everything to an image, vision encoder, text decoder, and you get JSON out. And it has, instead of just one big brick of text, which is kind of like what you have with the document loaders. Now, what we're doing is we're saying, taking snippets out and saying, Okay, this is the title and having a title metadata tag. And this is the body, body, text chunk, so on so forth. And so you can be much more thoughtful about one, what you're wanting to store, and then to what you're feeding into prompt, right downstream. And so again, everything at the end of the day is going to come down to speed and cost of compute. And you can chew up a lot if you're doing this in production for an enterprise without without having done that work on the on the on the front end. And so the goal then with this with this larger model, then is take huge volumes of heterogeneous data. And then you can curate it very thoughtfully, before you decide to go spend a bunch of money on all of it within LM and like, Jerry over at Llama Index, he had a great piece in medium just, just last week, looking at that Claude and 100,000 token window. It's a dollar a prompt almost every, every time you query the model. I mean, if you're doing this across an organization, right, like that's going to add up in a hurry. And so what could you do to maybe avoid having it look across 100,000 tokens? And do you want to right?",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, and that brings me to one of the topics I'm most excited to talk to you about, which is this kind of like, extracting structured data from unstructured data. And to as a quick background, like with we've yet vector search, in addition to doing vector search or hybrid search, we also have where filters. So if you want to say like, you know, where web page equals weaviate.io or like where price equals less than or like prices less than $100, like these kind of symbolic filters, they integrate with the HNSW vector index to facilitate using this kind of structured data in addition to searching through the text chunks or bricks. I think that I like that abstraction a lot thing about as bricks. But so I wanted to get your thoughts on this one paper, and I'll give a TLDR quickly, but just in case, you know, for everyone out there. So this paper is called evaporate code plus is the name of the algorithm. And so what you do is say you're looking at Wikipedia pages of NBA players. First, you look at a subset. So like Kevin Durant, Jayson Tatum, LeBron James, and so from you look at this whole thing with an expensive large language model, take out the symbolic attributes like college, height, years in the NBA, let's say, and then you sit and then the large language model writes Python programs to extract those feature those attributes. And then you generalize the Python programs to doing the rest of the Wikipedia pages. So hopefully, that was a good explanation of the technique. And if it's not unclear, I'd be happy to clarify. But do you think about that kind of like, bootstrapped, like, here are the attributes, and then here's a way to extract the attributes from unstructured data? ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Yeah, I think, um, once you get it into that clean JSON, that's prepped and ready to go, like, we think there's a lot of complexity in connecting to everywhere that an enterprise or an organization stores a lot of this data, and then long tail file formats. And so we've been less focused to date on trying to adapt like GPT-4 or a Bard or something like that, in order to do all of that. Because over time, we may pivot if the compute efficiency improves and like a performance improves of that to say, okay, let's, let's build an ingestion pre processing platform from the ground up to do that, get it into that base format, and then use LLMs on top of that. On top of that, that data, it's just going to be a lot faster and a lot cheaper. And it's going to be more performant for the most part right now. And I think you're seeing some of these challenges with like agents, right now and moving them into production, like incredibly promising, incredibly exciting. Nobody's using them in production really yet, right? And it's going to be a while. And so, like, we welcome open source community continuing to like, to work on that. And like, as they progress, like we may, we may pivot on our strategy, but we're most bullish on putting the LLMs against the clean JSON. And then asking, because you're just asking a lot less of the model, which means that that the structured data that comes out the other end is going to be a lot, a lot higher quality. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Gotcha. I think I'm starting to understand. But so so say I have like, you know, like parquet files, or CSV files, or, you know, maybe I have like a graph database, like I have some data in Neo4j. And I come to unstructured and I say, Hey, could you transform this into, like, I need a weaviate database, could you turn this in? Could you like read the documentation and learn about how are you transform data from one file to another? Is that kind of the general thinking? And so could you could you shed more light on what exactly that looks like? Because I'm sure you have a lot more knowledge about that thing. ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Yeah, on our side, we've done a lot of work on staging bricks on like, on like JSON or markdown transformations. And so once we get it into a particular base format, or be adapting it to what's needed, and this is really, like last year, we were doing a lot of work around like, label studio and snorkel and label box and like long tail labeling, you know, vendors, and then also some of the NLP vendors, and everyone had like a slightly different JSON schema requirement. And so we were doing a lot of work on these staging bricks, which would take a base one and then adapt it to the the schema that's required for that particular downstream application. And we're probably gonna need to do continue doing more more of this. But again, like our, our kind of, you know, our strike zone is, okay, I have a file that contains natural language, and I need to get that into a consistent format. So when I can curate it, for, you know, to create a knowledge graph or something like that, right. And so that's, that's really kind of the sand trap for data scientists today. And we're, I don't think we're, we claim that we've solved it by any means. But we're, we're, you know, we're running hard against that, that particular problem set. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "It really, I mean, I think like, so, so if I could understand the problem a little better, so I know things like, you know, say I have some kind of special character in my unstructured text that when I try to decode it, it's like, you know, ASCII can't decode this character is a mostly thing about problems like that, like where you're ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "So is it correct? Like, I'm thinking about maybe like with like, how with like, say, like no SQL systems, you have like this evolving schema, you can transform your schema as your data kind of evolves, and you're adding new properties, attributes, and then it's kind of like the syncing up of that at massive scale. It could be one other part of it. ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "But what we've done is we've come up with our own ontology, just mainly because the one hat doesn't exist yet of documents, though, instead of like, um, of columns and rows, it's like, okay, like, we featured a lot of thinking that's like over the winter, the spring, right? Like, we don't we need a bunch of categories, but we don't need too many categories of, of like, you know, what are common across receipts and PowerPoints and memos and right and like, in the natural language domain, we're like, you know, some of our I mentioned already, but like lists and headers and footers and advertisement and captions, all these things. And so what you end up with are tokens spans with a metadata tag to what type of document out like universal document element that corresponds to. And so you could say, Oh, I want to like exclude all of the headers and footers and captions from what I send to weaviate. I don't want those getting sucked up into what I'm prompting against, for example, right with an LLM. Or, you know, I want everything or I just want to isolate certain some things. And so you can that's, that allows like this rapid curation, rather than having to guess and check, you know, writing regex and Python scripts, and then changes them, the myth cloud categorizing it, you can do that effortlessly with our toolkit. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. Yeah, that quick list of the PowerPoints receipts that helped me help to click for me a little more just like, wow, like how much how much information is visually organized? I have to get like, I'm so biased in my own like experience of thinking just about like, you know, archive papers and like blog posts is the thing. But I mean, can you tell me more about just like what you're seeing with all the information out there? That's like a visual document layout, like you? I don't know, in PowerPoints receipts are pretty awesome. ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "I mean, the world that we like, the customers we're talking to are primarily large organizations. And just think about like, okay, how many data loaders have all the all the developers that are supporting llama index and maintain and others come up with right. And a lot of those are kind of like easier data. And then you have like, you have Salesforce and you have HubSpot. On the other hand, you have Slack and you have notion and then you have stuff that's out there on the internet. And then you have who knows what in G drive or in SharePoint, and then you have historical things that are in an s3 bucket, right that nobody's ever done anything with. You have recordings of podcasts and things like that, right? Run through the text. But the bottom line, all of this, like there's this whole world of private information that organizations are generating every day, that specific to their particular mission, right? And the like, the exciting thing here is like, how do we like for them? It's how do we take what's out there and publicly available? What we're generating internally? Or where like Weaviate and then utilize it in conjunction with an LM, either to enhance productivity or to make better decisions. And maybe we write memos a particular way, maybe we make presentations a certain way. Maybe there's this historical knowledge about how we do things. And I can query, you know, I have this vision of being able to query it. How do we take all that valuable data, natural language data that's everywhere, and then get it into Weaviate, for example, right and, and utilizing the text with LM like, that's what's so exciting for these organizations right now. But that's, there's some huge bottlenecks along that journey for them to navigate. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Amazing! I mean, I think about like how, like within our company, how information is kind of like visually organized and like notion and confluence. And, you know, maybe like even like a JIRA board, you could visually look at it and then parse it out from that. And even just like, as I was doing did that, you know, let's list out some things I started thinking about, like resumes that are visually organized and parsing that out. Yeah, it's all just incredibly interesting. So let me ask you about this kind of chunking and determining atomic units for text. So we generally with text embedding models, try to get it to around like 512 tokens. And some like with, you know, LangChain has this recursive character splitter thing as a part of their PDF ingestion, or like as part of their data ingestion library, where basically what it does is, you know, it like, if you have like, it has a few elements to it. So firstly, like if you have 1500 tokens, it'll like chunk it up the first 512 next 512, and then the remaining thing, send that all out to the vector database or whatever. Or it'll be like if you're trying to parse like a Python file, it'll like particularly have delimiters for like new lines or like def for the function, like these kind of little things. So do you think there's a lot of innovation in chunking? ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Yeah, I think like what we're, at least for on our end, I think Harrison's on the right track with, with the recursive text splitter, I think we're, we're, we're going with that is chunking into increasingly smaller, smaller units that have the metadata tags associated with it. And so you're not like, let's use a web page, for example, let's, let's say you scrape a New York Times, a long New York Times article, right. And what will happen right now is like, that'll get chunked into, you know, 512 token spans or something like that, right. But within that, you'll have advertisements, you'll have links to other articles, you'll have image captions, you'll have, you know, all sorts of stuff, right, maybe a text box about the author or about a related article, and all of that. In an ideal world, like, if you're if you're just doing chat your data, it probably doesn't matter as much if you're wanting to continuously update knowledge graphs, or like Salesforce or something like that, using LLMs matters a lot more. And so what you want, right, is to say, okay, here are these, here's this chunk of like, maybe body text that said that's split up by 512 tokens, right. But then I know that this is an advertisement, I know this is a text box and talking about something else. And I know that, you know, these other elements aren't necessarily related to the topic of this article right here, that's corresponds to this headline. And so you can just do a little bit of work on the front end, like through unstructured, so that you're not asking nearly as much of the LLM and that when you're ready to turn the corner from chat your data to continuously update knowledge graphs or automate, there's going to be a lot less noise going into that data itself.",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's super interesting. And I really I do want to like, so you mentioned knowledge graphs. And this is probably like the, the most use of structure is to, you know, connect each chunk to how it's like related to another chunk in this kind of thing. Well, earlier, I had mentioned this kind of idea of just adding symbolic filters to your chunks and then using Weaviate to search through these symbolic filters. And then like, it reminds me of say, like the text to SQL kind of research where you could have an LLM generate a query that adds symbolic structure to the semantic search query. But broadly, what I want to ask you about is, what is your sentiment on knowledge graphs? Do you think knowledge graphs are, you know, like, if you're betting on knowledge graphs as being more or less important as you know, in the next three to five years? ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "I think like what we've seen in the last six months has been the fact that like chat your data has captured everyone's attention. And I think that that's going to continue and it's incredibly exciting. What I also think is that for a lot of business processes, and organization processes, you need a high degree of precision in order to hand things off to automation. And that is in the realm of, of knowledge graphs. And so if it's classification to like, you know, in a or topic modeling on the one hand to entity extraction, to relation extraction among entities, and then and then more, right? Those are still like, there's still a ton of manual human work being done in those areas that, like the previous generation transformer based models were good at but really expensive to set up and really brittle. And that like, you didn't see widespread adoption of NLP across like fortune 50 fortune 500 organizations, in large part, because there's a huge economic problem around it. The economics are changing in a really positive direction. But, but with it's still not a solved problem. And so I think like we have a renewed opportunity as as an NLP community to go and attack that and to drive that value, but that value has not been delivered yet, by and large, right? It's been it's prototyped, and it's been described. And there's been lots of cool Twitter demos. But like, there's still 1000s and 1000s of humans doing things that that model can do well. And so now we have a chance to go back and re-attack that. And so I think you're gonna have like, there's just, you know, the generative tasks and and others where chat your data or prompt your data is better. But there's still a lot that like structure, like the creation and maintenance of knowledge graphs are necessary if you're going to hand it off from humans to to models. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "I think, yeah, so with with the knowledge graphs, I'm just really curious, like, if we could unpack a little more of when you need to use a knowledge graph, I don't mean to be like, because I'm just kind of skeptical, honestly, I'm like, to me, I think the best thing about a knowledge graph is that if we link together the how chunks are related to each other, maybe we can use some kind of graph neural network to kind of have embeddings flow through the graph and maybe get a more contextual sense of the embeddings. That's more so how I like a knowledge graph. Can you tell me just like about when you query like, because you have the relational tuples like Connor, and then you know, all the things about me, says an example, can you tell me more about like, how you query knowledge graphs and how that's useful in business? ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Yeah, I think, look, there's events and there's entities, right? And entities can be organizations, they can be SKUs, they can be like, supplier relationships, there's all sorts of things, right. And then there's all sorts of structured data associated with that relationship to meaning like, what are key events that happened? What are key transactions? Lots of things that are described in natural language that aren't that might not necessarily be in, in a structured format already, that you need to go attach to that record, right, if you're actually going to hand things off. And so let me give you an example, we were talking with an investment advisory firm several months back. And they advise like endowments and others on investment. And then and they themselves are receiving every month, hundreds of limited part LP reports, McKinsey reports, like some like consulting reports, like all sorts of data, some kind of in a gray area, if it's private or not some really private. And they have a whole team of people that are just reading them and then just updating them into a CRM so that they have like 100% accurate information on any particular business entity at any given time to inform investment decisions. Yes, you can put an LLM on top of that and ask questions. But are you we're going to trust it with $100 million decision? Probably, probably not, right. And so if you're going to feed in all of those PowerPoint decks, the McKinsey reports, those LP memos, all that sort of stuff, and you want to create 100% accurate or 99.99% accurate records about certain securities, organizations, you're going to want like very high quality knowledge graph associated with it, even though you could chat your data. How the LLMs demonstrate that level of performance of the previous generation of transformer based models, like it's going to be a challenge, right. And so that's, there's, that's probably the more common NLP use case today, then Goldman Sachs want to chat all of their data across all of their divisions. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, amazing. I mean, that example really like lit a fire under my ass of the thinking about I'm Goldman Sachs. And I know the like the hallucination thing of you give it the thing that hallucinates. So how are you thinking? I mean, it's a great example of a hallucination case where you just absolutely cannot hallucinate. Maybe we could talk about this topic quickly about the innovations and hallucinations. Like, it seems like there are some things you like as the language models get better and better, it's becoming less of a problem that maybe I could just, I guess it's gonna do things like with the knowledge graph thing. It's like, do is like, do I need these tuple like it's entity relation entity, this is how I should have my data structured in order to hand it to the Goldman Sachs LLM? Or can I just have it kind of be a sentence, like a natural sentence that comes out of a retrieval engine to hand to the large language model? ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Yeah, I think I mean, you deal with a lot of like type one and type two problems here of like, hey, did I is what's being generated anchored on something real? Or on something, you know, that that bears a significant resemblance? Or the other thing is like, is it missing something in the generative side? Right? Is there an unknown unknown, right? And so, like, both of those are critical to actually saw like putting this into production. And so I think like, by and large, hallucination has is a lot less of a problem than it was than we thought it was maybe six months ago. From like an open source developer community perspective, from a production perspective, this is an area ripe for innovation. Because we might be able to make sure that like what is generated is anchored on something real. But I haven't seen a whole lot yet around making sure that you have a totality of information of the right information being rendered for you. And like, look, you can look at like the you know, like Google's LM versus OpenAI LM and some of the horse racing different past few days between them on on what they're returning from prompts from very similar prompts. And it's still early days, right. And so that's not to say that these are incredible, but like, you know, there's I think there's still a chasm between pure like Jasper AI generative applications and replacing humans that are doing knowledge generation work right today. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, amazing. And yeah, so I think that was a really great coverage of all these topics. I think what you've hit on with the particularly kind of the visual document layout is what originally drew me to instruction, I think you have hit such an important part of this kind of flow of these retrieval augmented LM vector database all the whole world that we're in. And, and yeah, it was so interesting learning about all how you think about all these things like the data connector, like, you know, flowing data from say, like parquet into the JSON, as you described, all that is so interesting. Before we wrap up, maybe do you have any like exciting upcoming announcements or any tips of viewers for where to catch up with you? ",
        "podNumber": 48
    },
    {
        "speaker": "Brian Raymond",
        "content": "Yeah, I'd say, um, you know, we're in our community slack, welcome others to hop on in we have our engineers are in there full time answering questions, the ones who are building are answering questions. Correct. And so we have an amazing team. Keep your stay tuned for some new models to be dropping over the next eight weeks, we're going to have a good steady drip of those. And just welcome feedback. We're trying to build through real problems through real real users and, and leverage the power of this open source community. And so I'm grateful for any contributions or feedback anyone anyone provides. But Connor, thanks for having me on. ",
        "podNumber": 48
    },
    {
        "speaker": "Connor Shorten",
        "content": "Thank you so much for your time, Brian. And yeah, you mentioned the suite of the new extraction models, we are definitely going to be on top of that. And we've been trying to, you know, get help people get their data into we've been using these tools from unstructured. And yeah, thanks again. Thanks.",
        "podNumber": 48
    }
]