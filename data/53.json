[
    {
        "speaker": "Connor Shorten",
        "content": "Hey everyone, thank you so much for watching another episode of the Weaviate podcast. I'm super excited to welcome Stephanie Horbaczewski and Gunjan Bhattarai from Vody. We've been following on with Vody for a long time at Weaviate. They're building all sorts of super exciting things with multimodal AI, combining different modalities from text and images, this kind of idea. So this podcast is going to dive all into that topic and learn about what Vody is working on. So Stephanie and Gunjan, firstly, thank you so much for joining the podcast. ",
        "podNumber": 53,
        "clipNumber": 1
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Thanks so much for having us, Connor. You were one of our first startup friends. We're really excited to be here. ",
        "podNumber": 53,
        "clipNumber": 2
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Yeah, thanks so much. ",
        "podNumber": 53,
        "clipNumber": 3
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. So could we start off with the story of Vody, the problem its solving and the founding vision? ",
        "podNumber": 53,
        "clipNumber": 4
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yeah, definitely. So I'm a second-time founder. My first business was a $100 million a year business on top of Google and Facebook that worked entirely with unstructured data. So when BERT came out, I grabbed a couple of my technical team and left, invested some of the money I made and started a research company. We did three years of projects working on embeddings, worked with Comcast, HBO, moved into retail with Nike, and then spent a year developing our product with Wayfair. ",
        "podNumber": 53,
        "clipNumber": 5
    },
    {
        "speaker": "Connor Shorten",
        "content": "Wow, amazing. So if we could unpack that a little bit. So you were already working with unstructured data, you saw the BERT revolution, and then you instantly said multimodal is next, I think, or was it you grew into multimodal?",
        "podNumber": 53,
        "clipNumber": 6
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "What really happened is I said recommendations are bad and why? I didn't understand. It felt like there was a ton of data and it didn't make sense to me that we were returning such poor recommendations, which is when I discovered that what a customer is using to make decisions, for example, images, Q&A, reviews, structured data is not what's being used to make those recommendations. And so I thought if we could bridge that gap, we're going to have some really transformative experiences in e-commerce. And so we set out to build those specific models, fine-tuned for the e-commerce media space, multimodal models to use all these different types of data. ",
        "podNumber": 53,
        "clipNumber": 7
    },
    {
        "speaker": "Connor Shorten",
        "content": "That's so interesting. I've been dabbling with recommendations and building apparel recommendations to illustrate our Ref2Vec feature, which is a way that we've built in to have recommendations with embeddings. I recently at ODSC East listened to Madhav Thaker describe how they use embeddings at Shopify for recommendation and his explanation of the business impact of that was incredible and understanding the scope of it. So, maybe leaving on the business CEO hat, you described new experiences with recommendation. Can you kind of unpack that a little more? ",
        "podNumber": 53,
        "clipNumber": 8
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yeah. I mean, what's amazing about the embedding is it's bringing all of the understanding about a product. So, a real complete human level of understanding is going back to your model to make recommendations. So, when you're looking for a fuchsia chair with wooden legs, there's not enough data labeling in the world right now to do that. But if you have an actual understanding of the product, and so Gunjan, maybe you can talk a little bit about our multimodal product embedding. ",
        "podNumber": 53,
        "clipNumber": 9
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Sure. So, I'll go a little bit further on the fuchsia chair and be like, most models simply don't even have the capability to handle that today. And so, some of the core concerns of that is, for example, one, you have to detect color in an image and that involves vision. At the same time, though, there may be multiple objects in an image. Maybe there's something specific in that image you want the color of. And so, you sort of need to be able to guide the model as to what you want and that would involve natural language. So, that's one of the core opportunities in multimodal. Previously, we'd only have some understanding of how things might look from an image perspective or a text perspective. It's like someone who was previously illiterate but had perfect vision and then some person who had a PhD but was entirely blind. And so, we're able to combine these two extreme forms of knowledge into one person and basically have a model that knows all the latest advancements in science and technology and like all sorts of disciplines, but is able to actually see and understand how the world looks like with their own eyes. So, multimodal embeddings essentially deliver on that promise. So, the idea is that products are multimodal in nature. They have textual features, for example, product descriptions, product titles, reviews. They have images, namely the images that you're trying to have someone sell for and other details to, like various metadata fields, things related to what other products people buy, maybe some product videos, lots of other things. And so, the idea is that we can jointly align these types of representations together and be able to have a model that understands all of these. We're able to have embeddings that are able to understand these products really well and can understand what's similar, what's different, and then people can build models on top of them and build search that's able to leverage that newfound understanding. ",
        "podNumber": 53,
        "clipNumber": 10
    },
    {
        "speaker": "Connor Shorten",
        "content": "Fascinating. I want to park the topic of the color thing. I think that's really fascinating, this idea of how you use classifiers to extract metadata from unstructured data. That's definitely a topic as we go into the weeds further. But coming, I really want to understand Vody a little better. So, this decision to do custom embedding models, that's a really unique angle. I think OpenAI and Cohere, they offer a custom text embedding model. Right now, I'm not so sure that the tooling on doing a custom embedding model is all that strong yet. So, it does seem like such an interesting market category to go after. Can you explain a little further this custom embedding models for people, what that looks like? ",
        "podNumber": 53,
        "clipNumber": 11
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yeah. So, to get these amazing, incredible open source multimodal models ready to be used in production and in an environment in e-commerce, you have to do a lot of things to them. So, you have to build extensive data sets very specific to this. You have to build different architectures to the different modalities, optimize them for them. And maybe Gunjan, maybe you can give them a few highlights of what, for example, we did with our first model. ",
        "podNumber": 53,
        "clipNumber": 12
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "So, essentially, with our first model, in order to make these embeddings work, we had to first understand, okay, these open source models are really good at, say, generic images and text, stuff that you can find easily in the internet, stuff on Google Images, basic stuff. The problem, though, is that these companies usually have very specific domains they work in. For example, e-commerce companies like Amazon or Wayfair have very different requirements of what they want a model to look like than maybe standard Google or Microsoft, because e-commerce is inherently different from standard multimodal. And so, a model would need to be domain adapted to be able to understand what these types of products look like, what text, when you try to sell a product, looks like, how products look like, the prices of these things. Basically, there's new details that these types of models don't understand. And so, one of the most important steps is to domain adapt these models so they are able to understand e-commerce at a meaningful level. And this can be extended to basically an industry, real estate, AR, VR, plenty of others. So, that's one of the most important steps as to what these businesses need in these types of multimodal embeddings. But other things, too. ",
        "podNumber": 53,
        "clipNumber": 13
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yeah, let's give us an example. You will give a few examples. You know, understanding what life stage your product is for, what room you might use it in, what style it might be. You know, if we look ahead to what you do with multimodal LLMs, imagine being able to go to an e-commerce site and generate a style blog about how you might style this item, particularly in your house. This weekend, Jonathan and I spent a bunch of time on midjourney and chatGPT. So, in essence, creating a way to style some different ways to do our house. And so, you know, that's really going to be part of your new e-commerce experience. You're not just going to buy the products, you're really going to be able to understand how to use them in your life. And it's going to be really incredible. ",
        "podNumber": 53,
        "clipNumber": 14
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, that paints such a compelling picture that kind of, I've seen like these room GPT things on like Twitter and stuff. It sounds like that kind of idea. And we're going to come back to that. And maybe as a thing, if you're listening and you want to skip ahead to that, you can see the chapters in the video. But I want to stay on this custom embedding models a little further because I think this is such a deep topic. As an example, I like to kind of play with Weaviate by taking these podcast transcriptions and putting them in Weaviate, so Stephanie and Gunjan, you'll soon be in my dataset. And like, I find with this that, yeah, like the custom text embeddings are getting me way better search results than the off-the-shelf, any of the off-the-shelf models because of this specific conversation we're having as we start saying things like roomGPT, CLIP, instructBLIP. We started saying all these buzzwords, these keywords, and it's like the models, the domain adaptation to them. Can we unpack that a little more for like the multimodal e-commerce case? Like say I'm like Alo, like an apparel brand. It's like the particular style of my clothing is that you're going to start fine tuning the embedding models on the descriptions of the alo shirt with the alo shirt. And then that kind of leads to better search. Is that kind of the general thinking? ",
        "podNumber": 53,
        "clipNumber": 15
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yeah. So I think how you're going to use this is a good way to answer that. So by creating first an embedding of the product using all of the data you have, so you have this human level of understanding about it, putting it in your Weaviate database, and then eventually creating an embedding of the search query from the user. And so now you're matching up exactly, you know, semantic search, exactly what they mean. And let's take it further. When you want to start searching and say, oh, I really like this shirt, but I want something brighter and I want it short sleeve. And it understands what you mean and is able to identify that. Or our COO was just looking for a mixed drink maker. And he discovered as he looked through them that he wanted it to work outside, that he needed it to be able to handle alcohol. And being able to revise and, you know, have it understand that you've made these new decisions about how you want to, how do you want to search and how you understand the product. Gunjan, maybe you want to talk a little bit about, you know, that integration that they use to those two sets of embeddings. ",
        "podNumber": 53,
        "clipNumber": 16
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "In order to combine these embeddings, essentially, well, first, where are you going to use these embeddings? So in the case of something like product search, you have two options. Essentially, the core problem is that search is very, very speed dependent. And any and every speed optimization is going to be essential. And if you're even a second late, your business has lost millions of dollars of revenue. And so things that need to happen is you need to take these embeddings. Usually you're going to take your products, get embeddings of each of these, cache them and use them in some sort of vector database, like for example, Weaviate. And you can use those embeddings to be able to figure out what products are similar to each other. So you can therefore say, do on the fly recommendations to your customers with your website. So that's one valuable way in order to do integrations. Of course, there might be other ways, like if you're doing an offline application, maybe for example, you want to use these embeddings to train a model as Steph noted to detect the style or what room this product might be in. And so that might just be something where you do a data labeling task offline, use this model to add additional information, pass it into Elasticsearch or Cloud Search or other types of services like that, and essentially be able to use that information on your existing search without changing anything. So the integrations are going to fundamentally depend on the use case. In the case of e-commerce, speed is usually paramount. And so that's going to affect most of what you're doing. But essentially, it doesn't look too different from how unimodal models like in text or vision work within the e-commerce space. It's just dumping in a new model and everything you would have used before would be what you would use today. ",
        "podNumber": 53,
        "clipNumber": 17
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "I think there's a great example from the project we did with HBO that a consumer will really relate to. I loved Queen's Gambit. If I go to HBO and I say, hey, can you show me what you have that's most like this? Because they're using interaction data and very limited structure data, they don't know what I mean. And so building an embedding of all film and TV shows, what we did, we were able to work with HBO on how would they make recommendations about data that they have no interaction data on. Will they have a complete understanding with thousands of topical and emotional attributes? Then these embeddings understand the movies the way you and I do. Next, by the way, very exciting would be to embed the movie itself. But you could use those embeddings, initialize your model originally as the content understanding, and then no matter what we searched for on these platforms, we'd be able to see what they have that is most like that show. ",
        "podNumber": 53,
        "clipNumber": 18
    },
    {
        "speaker": "Connor Shorten",
        "content": "So kind of staying on the movie example, it reminds me of like... So I think maybe there are two things here. There's where you are labeling the categories as well as just the embedding by itself. The next step being the embedding of the movie itself that just captures all the semantics compared to where you classify each label and then that becomes the vector, it sounds like. So if I take a movie like Deadpool, that's like action and also comedy. So I have those two labels, it's not horror, and making that label. So that's how you then... It would bootstrap like a collaborative filtering kind of thing where you recommend in that kind of way. So it sounds... ",
        "podNumber": 53,
        "clipNumber": 19
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yeah. Yeah. Bingo. Actually, genres is something none of us can agree about. We can only agree on two genres. So example where while that seems like helpful structured data, it actually isn't. It makes recommendations due to consumer bias and how they have different opinions. What's funny? ",
        "podNumber": 53,
        "clipNumber": 20
    },
    {
        "speaker": "Connor Shorten",
        "content": "So a lot of it is training classifiers then as well, right? Then because you just... It's not like the contrastive image. I usually think when I think of embedding models, I think that you're contrasting it compared to this sounds like you're producing classifiers. Is that correct? ",
        "podNumber": 53,
        "clipNumber": 21
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Both. So it seems that the community has largely coalesced around using some sort of contrastive loss function in order to create. So for example, sentence transformers NLP based is done based on contrastive learning. And you might also find something like CLIP or BLIP or SLIP or all those other alternatives. Those, I mean, they all use the same naming convention. So essentially all those use contrastive learning in order to align image and text embeddings. You can extend that to other modalities as well, like including video and audio, like what you might want to do in the context of a movie. So contrastive loss is still going to be a key part, at least in this state of the art that I'm aware of. Classifiers matter too in that the way that you can also structure this contrastive loss as a classifier, like if you want to do using a momentum queue, you can style it that way. Classifiers also matter, for example, if maybe you have a lot of data that has a classification label and you want to use that in order to inform your learning. For example, maybe you know that these types of products belong in this category, or I guess in the context of movies, the type of movies belong in this genre. This other type of movies belong in this different type of genre. And so you can use that to sort of partition these embeddings away, and that can also be used. So I think mostly it's a contrastive problem. Classification is a way in order to get more data to help. You can structure said contrastive problem as a classification one, although it's one option out of many. Contrastive is a really big problem. And of course, you can fine-tune many of these embeddings in classifier type problems, which we anticipate would be a massive use of these types of embeddings. So kind of all of the above. ",
        "podNumber": 53,
        "clipNumber": 22
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, really. So I think we're starting to unpack the meat of how this, how the training happens and the fine-tuning. So I guess my first question with that is the thinking around fine-tuning from a foundation model and that kind of thinking. So does Vody, do you take a pre-trained model off the shelf and then start fine-tuning it for HBO, Nike, or do you train your own kind of foundation model, or you pre-train it on a massive amount of data set? How are you thinking about that kind of thing? ",
        "podNumber": 53,
        "clipNumber": 23
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yeah. So we build our own data sets. We have huge data sets for this purpose, and we are building fine-tuned models that are zero shot off the shelf. So you can come and take them. There are still a lot of hurdles, as you know, to overcome in the data space. And so we are making it very easy for our clients to come and take them off the shelf. We build our own models and we fine-tune others. Particular shout out for me to Jun-An Lee and Stephen Hoy and the team at Salesforce. Others have helped, we think their work is super imaginative and love working with it, but we work with all models. Gunjan, do you want to talk about some of our most recent architectures? ",
        "podNumber": 53,
        "clipNumber": 24
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Sure. So I built a bunch lately. And so essentially some of this might involve, for example, trying to fuse models together, like for example, images and text, you want to fuse them. You can either fuse them right at the beginning or at the end when you build a model. You can say, have some sort of clip-like architecture where you're not fusing them at all. You're basically just training two separate models or multiple models if you want to extend this to additional modalities and basically have them all represent similar ideas that are represented in text or image or some other modality. That's another option. You can also say, combine these types of ideas first, use this alignment before then fusing those together. That might be helpful, like for example, if you want to do a primarily image-based task and you want to use a text to help guide the image model to an answer, that is another alternative. It really does depend. I think most of the decisions end up being dependent on client-specific needs. Namely, for example, say how fast the model needs to be, how much memory you can use, how accurate does it need to be. Those types of trade-offs are mainly where most of the custom model building happens. Because unlike in NLP or Vision where there's so many models and you can use them for every specific use case, there's a super small quick version that gets the job done. There are gigantic models that get you accuracy and pretty much everything in between. But multimodal is so nascent and multimodal has so many different modalities that there simply doesn't exist anything like it in open source or even closed source at this point. And so we have to build those out. ",
        "podNumber": 53,
        "clipNumber": 25
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "A couple domain-specific examples. Most Vision models take an average, which if you're talking about a flower or a living room is a great idea. In e-commerce, it's not. Usually the first shot is the product. By the time you get to three, you've got a detail shot. Maybe you even have a size chart. And so taking an average no longer works. One of the important things in e-commerce is you can't have to retrain this model every time you update your catalog. Some of these guys are updating their catalog literally hourly. And so they need to work. And so there's a lot of... Maybe, Gunjan, that's interesting to talk about. Maybe some of the InstructBlip work to do that. ",
        "podNumber": 53,
        "clipNumber": 26
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Sure. So if we want to talk about InstructBlip, essentially that would be sort of like taking these ideas to the LLM space. So GPT-4 proved that it's possible to build a multimodal LLM that takes Vision as input or text as input and is able to generate text based on that. There were other types of works before that, like Flamingo and plenty of others that don't come to my mind at the moment. But since GPT sort of generated this new generative wave, no pun intended, there's been a lot of other work to try and, say, replicate some of those innovations. And one of those was Instruct Blip, which first tried to train as efficiently as they could a way to take Vision as input and be able to generate text based upon it in the LLM world. And then Instruct Blip was like, okay, let's go take every academic dataset we can find, turn it into instructions, and teach this LLM that knowledge so it can then generalize to new tasks in a zero-shot setting that we previously may have not thought about. Since academic datasets only cover so much, if we have a model that can generalize sort of like chatGPT can, then it can do a crazy amount of things. So that's the essential idea. It is definitely in Vody's interest to be able to produce something like that in the e-commerce space. And further compounding that is the fact that every different company in e-commerce, or say in the case of film and TV, have different things they want to do a model with. And so their tasks are going to be fairly company to company. That's a problem for us since obviously this isn\u2019t a consulting firm. And so as a consequence, we want to build stuff that generalizes really well to other firms. And so this type of instruction tuning and having really good zero-shot performance that can generalize to new tasks that nobody at the firm even thought about would be really important. The idea of using instruction tuning to generalize to new sorts of tasks is something that allows for scalability. It allows for this model to be used in many different cases and makes us really excited about the potential of what these models can be used for. ",
        "podNumber": 53,
        "clipNumber": 27
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, without question. So there's a lot of information in that quickly, Stephanie, you mentioned the re-indexing as you update the catalog. And to just pull that nugget out of the podcast for people listening, that's a huge problem, re-indexing. I think we see that all the time and it's something to tackle. But yeah, I want to stay on this. You mentioned GPT-4 being able to handle multimodal things. We saw papers, like you mentioned, Flamingo, we saw Frozen, this paper that was called like pre-trained transformers as universal computation engines. Basically that you can just put the image embeddings into the latent space of GPT-3 and then it seems to be able to just instantly be able to reason across the embedding space, which is quite astonishing. But then, yeah, so I mean, this idea of foundation model training is so interesting. I think at the time of recording this podcast, Cohere has just announced raising like $270 million. And so this space is emerging. So do you think that the foundation model providers for multimodal, whether it's... And there's two things I want to parse out quickly. So there's the generative models, the multimodal generative models, and then there's the decision to train embedding models versus generative models. So can we start with, do you think the multimodal foundation generative models will be different from the text models? ",
        "podNumber": 53,
        "clipNumber": 28
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Yes. Namely because they're going to have a lot more capabilities, right? So for example, yes, you can say use clever tricks to try and pass a purely text model information about an image. LaVa, for example, basically is able to get surprisingly good performance from chatGPT, which has never seen an image before, by passing some context on the image, like a caption, as well as say bounding boxes, determining what every object is there. And the model is surprisingly good at creating data and generating it, which is helpful, except there are some limitations of that. For example, one, what we found is that using an object detection based model ends up slowing things down a great deal. It's not efficient. One paper called ViLT was able to remove that part and claim a 60 times increase in speed from doing so, which is super impressive. And there's also the idea that these types of multimodal models, because they're able to learn from each other, are able to get a much more vivid picture that is able to integrate these two much better than say what a text model can do with say some clever ways of injecting image information without training it. So for example, bring back in something I talked about earlier in the podcast, we have someone who basically only has seen images like with their own two eyes and seen the world around them, but is illiterate. And then we have someone who is blind and basically has the knowledge of practically every PhD in the world being visionary. Those two collaborating together, it's going to be a very difficult way to work together. They're not going to be nearly as good as if one person had those capabilities together. And the idea behind having a multimodal model is you want to have the one superstar person who can see the world and know everything about it and have the knowledge of all Wikipedia, as opposed to two people with these extreme split apart skills, trying their best to collaborate. So that's the opportunity in multimodal, being able to combine those two together instead of just limping your way with two extremes. ",
        "podNumber": 53,
        "clipNumber": 29
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Exactly the way you are as a consumer, Connor, right? When you're looking at buying a product, you're understanding that structured data, the price, et cetera, while looking at the style and the image and what it's going to look like, and you are processing them in the same space. And so it's the same idea. ",
        "podNumber": 53,
        "clipNumber": 30
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, brilliant. I agree fully with that, that certainly there's something to be doing the multimodal optimization. And yeah, I think just that putting image embeddings into a text model, the fact that that works at all is already pretty astonishing, right? ",
        "podNumber": 53,
        "clipNumber": 31
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "I was surprised. I would have never guessed. ",
        "podNumber": 53,
        "clipNumber": 32
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. Yeah, and I think, how could you have a good sense of audio or how songs sound from a text space? It doesn't make any sense. So certainly like that example is obviously visual and that makes sense too. So could we take apart that other thing, the decision to train embedding models versus generative models in this space of being a foundation model provider?",
        "podNumber": 53,
        "clipNumber": 33
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yes, building a model library where we actually have a model roadmap to do all of the things we're talking about. I think probably the answer to that comes more from the startup enterprise meetup, which is what is the more pressing need? And and right now we're building the models so that they can get more out of the models they've already deployed by including more data in them. And so while working towards sort of the future of introducing some of those, like a multimodal LLM and the product features you would need to build to support that model on the site is even different than the work, right? ",
        "podNumber": 53,
        "clipNumber": 34
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Yeah. So essentially embeddings in generative tend to serve two key different purposes. Embeddings, well, has a unique advantage of you'll be able to build services on top of them without changing it. Like if you want to do product search and recommendation or movie search and recommendation, you can use these embeddings, do semantic search, be able to cache them in a vector database so that you can do efficiency gains. And that tends to be the best way to do things in sort of speed limited applications. Embeddings also have the advantage that you can train task specific models, have much more smaller ones that are faster. And that would be the core advantage of embedding based models. Generative models tend to be much larger. They are much more generalizable, are able to do a crazy amount of stuff that even people who are well familiar with them still get astonished with every single day. And that is a valuable advantage of them. So they can do crazy things, can generalize to a lot of things. If you want to like cover some new tasks you didn't think about, these models can do that with astonishing ability. So that's the idea behind generative, being able to generalize to new tasks, have crazy good performance, and sort of like hit that accuracy frontier for customers who care about it. From embeddings we're most concerned with more traditional applications, speed related things. And so it's sort of like a dichotomy where different customers are going to want embeddings and different ones are going to want generative just depending on what their specific use case are. They don't really overlap simply because it seems that most businesses have very specific KPIs they need to hit and they tend to already be pretty demanding on them. And one does not really work for the other and vice versa. ",
        "podNumber": 53,
        "clipNumber": 35
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, it's so fascinating. I think like a couple notes I had, first day I was thinking about how we're talking about kind of classifiers as well as sort of the more open-ended like similarity search or generative. And I think there's this paper called SetFit, which is about how you fine tune embedding models for classification and it's more efficient and really efficiency and the cost of these things. That's the big topic I want to stay on a little further. I think training embedding models, embedding models generally aren't that big models. Like I think the OpenAI, Ada, they have some embeddings that come from that big model. But I think mostly embedding models, they're not big models, correct? Like around 100 million parameters. This is the typical scale we're talking about. Is that correct? ",
        "podNumber": 53,
        "clipNumber": 36
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "You can go a little bit smaller than that too. The most popular one in HuggingFace is like the smallest mini LM. It's like 21 million parameters. But you're right. Essentially, embedding models tend to see saturation. I think you don't get more than 300, 400, but honestly it's been a long time since I've checked the HuggingFace leaderboard. And so viewers are more than welcome to go check and figure out what the true numbers are. But there seems to be a saturation point where you don't really get better embeddings after getting beyond some specific model capacity, which suggests that embeddings are not necessarily that difficult, relatively speaking, for a deep learning model to learn. And so, but they don't really get any benefit from additional parameters. Generative, on the other hand, keeps on going better and better for as large as I've seen. I haven't seen any saturation point in generative yet. Maybe some people at OpenAI have found something. Maybe that's why they haven't gone past 175 billion. Although maybe GPT-4 is larger. I don't know. Maybe someone does. I would love to know. But yeah, just that embeddings tend to hit a saturation point where they don't get better, probably because embeddings and understanding something is a much easier task than being fluent and writing entire essays on some really random topic, which chatGPT does great. ",
        "podNumber": 53,
        "clipNumber": 37
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah. That to me is just one of the most interesting topics because if you're someone who's been studying deep learning for the last few years and you have all these skills around training models and MLOps and all that kind of stuff, I think training embedding models is the opportunity because they're smaller scale, you can apply all the clever training tricks, like as you mentioned, like programmatic labeling and knowledge distillation. This is more of an opportunity to apply that. And so, this brings me into my hot take. We'll see how well this ages. I've been thinking a lot about this new class of zero shot, the retrieval augmented generation kind of pipeline where the reasoning part and the search models are separate and you don't think about training the zero shot reader model and you only would think about training the search models. Do you think that that kind of paradigm, that that will be the way going forward? Because I guess the alternative view is that as it's getting cheaper to fine tune the language models, we'll say like the LoRA low rank adaptation, this kind of thing that the thinking is you don't even need to do retrieval augmented generation. You could just fine tune the language model or the multimodal generative model on your particular domain compared to this other setting where you're just updating the search models and you keep the zero shot generative model fixed. Do you think that'll be the way going forward? ",
        "podNumber": 53,
        "clipNumber": 38
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "So, I mean, there are very easy ways to build that today and is something that's going to be very common in applications. I myself have used that to great effect, which has been very fun. I'm weary about making predictions of what happens going forward since honestly my track record of predicting the future is less than I would like. I would be persuaded that a retrieval based augmentation system would help. For example, Bard is using that, chatGPT in their plus form has used that to the form of their plugins, and it seems like people are going in this type of direction. I think in this case, you're more talking about situations where you have a bunch of documents or a bunch of products or a bunch of movies and you sort of want to pick the one that's most relevant to feed into your LLM or whatever is doing this generation. And that I think is going to be valuable as long as say attention remains expensive. And there have been some massive innovations in the space for the last four months. This has been something I've been following really closely. I've seen more valuable stuff at an exponential pace in the last four months than I saw in the last three years. And I'm just astonished. And so maybe these types of things may become less relevant as attention becomes more efficient and can generate up to like a hundred thousand tokens, 200,000, and these types of crazy numbers. But those are also complicated to build out. And I don't see these systems integrating that for at least a few years, thanks to the fact that diffusion of innovation takes a while. And so maybe that, because of that, I can see this paradigm being useful for the next few years at least. And even with like these efficient form of attention, they are using retrieval as well. So they're trying to retrieve from a database in order to try and make things more efficient. So maybe even the future of attention, which is supposed to solve this problem, will literally be using retrieval in order to solve it. So we might see it just go in a different way. We'll just change our idea of attention. And I think retrieval is valuable, and maybe that'll even become the future of attention. I'll just stop at that. ",
        "podNumber": 53,
        "clipNumber": 39
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, there's quite a lot. I mean, like there's like the... If we want to... Like staying on this topic of like the RETRO models, where you're doing like fusion-in-decoder and you put embeddings directly into the attention layer. And I think we also took our top... There's like the LoRA, there's like the sparse fine tuning that just makes it easier to fine tune the existing language models. And then we're coming into like these new models, like Mosaic\u2019s MPT, or the Anthropic Claude, that they are designed to take massive input windows. And I think that complements retrieval quite nicely. But sort of stepping outside back into our recommendation hat, I'm curious with this perspective on recommendation with embeddings. And then because the way that I see recommendation, that it's typically been done by these symbolic ranking models, where you take features about Connor, like male, likes basketball, maybe that's a feature. I like basketball. And then you would have features for each of the Nike products. And so you'd feed these into like XGBoost and rank them. Do you think those kinds of models are still valuable? Does Vody think about training those kinds of models? ",
        "podNumber": 53,
        "clipNumber": 40
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "So I do think they are valuable. For example, one focus of ours is to look at structured data fields, seeing they have a lot of context. And those structured data fields tend to align quite nicely with say, person is male, and like these specific types of like, well, I'm an NLP at heart bag of words type features. So essentially, I do think that's still going to be valuable. I think at least in the context of e-commerce, and maybe like things like Netflix, it goes beyond say content-based embeddings. And you also have to start considering what other people want. And that would, or what people want and what similar things are, what similar movies, similar products, that tends to be really valuable to do recommendations in this space. And so I think recommendations are going to be heavy on that. But in order to even process and understand that information effectively, your model needs to have the full picture of like how to use that information. It doesn't matter if you have like a billion, like clicking of your movies, if say, you don't know how to use them. Like if all I know is movie, name, title, and I'm trying to figure out clicks of like a billion from that, I don't have much information, even like find patterns, but multimodal broadens that scope a lot. And now I can use say, specific parts of that movie, I can use images, the trailers, the entire transcript. Now I'm able to broaden what I'm able to use to process those people that like a billion people that clicked on your movies to be able to make recommendations. So this doesn't really change the fact that seeing what other people want and seeing how similar you are to other people, that is still going to be valuable. But what multimodal allows you to do is be able to take that information and use a lot more of it than you were able to previously. So broadening the view. ",
        "podNumber": 53,
        "clipNumber": 41
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think, well, yeah, for sure. It seems like the embeddings from the products alone can get a lot of that symbolic data out of it. And then it's getting so good that it captures it all. Like if I click on a few pictures of, I'm on Nike and I start clicking on like LeBron James shoes or like a Kyrie Irving t-shirt, and now it can pick up on all these other kind of features for me pretty quickly. So earlier we mentioned RoomGPT and that we'd come back into that topic later on. So I'm tagging it now. So this kind of idea where you retrieve and then generate personalized retrieval. So I take Connor and maybe using the Weaviate Ref2Vec example, just because this is how I think about it. But I'd have Connor and I like this LeBron James shoes, Kyrie Irving t-shirt. I average those vectors and then that's the Connor vector. And so now I search for the new products and then I would feed those new products into a generative model where it's like a picture of me and it puts the shirt on me and stuff like this. How are you thinking about that kind of personalized generation? I hope I set up the question correctly, but this kind of thing where you like, to come back to the RoomGPT, it's like I take a picture of my living room and then the new products come in as a big catalog of like a thousand Wayfair tables and stuff. And then first it ranks it by using the Connor vector and then it puts it in my room so I can see what it would look like in my living room. That kind of thing. ",
        "podNumber": 53,
        "clipNumber": 42
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Yeah. So I find Room GPT interesting more so because it uses ControlNet, which I found interesting since you're able to condition that generation of say what you want a room to look like and be able to better control how say your text to image model works. That I find is really interesting. Of course this is relatively nascent and ControlNet is a relatively new model. I mean, I think 1.1 came out recently, but so I think it's going to be a very interesting space going forward. I think there's going to be a lot of innovation. I think it's cool right now. Definitely in the context of say e-commerce, you will want to like when you know what room something wants to belong in, maybe there's multiple valuable rooms that an object might be relevant in. Lamps can be in different types of rooms. And so if you can use this sort of like text to image generation with ControlNet or I guess more generally RoomGPT, you'll be able to or a retailer could easily just take this and change their lamp in a living room to maybe a lamp in a dining room or in a bedroom, all with using a simple model. And I'm sure that will be a very valuable place for being able to sell these types of products in a way that more is customized with customer needs. So I think it's really valuable. I think it's really new research and I'm excited to see say what's going to happen the next few months because I think a lot's going to happen. ",
        "podNumber": 53,
        "clipNumber": 43
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "I think you'll be able to index a little lower on personal information, right? So like we won't need to know as much about Connor if you're putting in a room and you understand so much more about every single product that's in the room. And if you're saying, hey, what could I put on this wall, right? You'll be able to use so much more context from the image itself and rely less on the user. Netflix does some incredible work with embeddings for everybody who wants to read more on their blog and they're able to look at viewing patterns. So again, thousands of topical and emotional attributes and say cut different trailers to show different audiences, reorder, they're even personalizing the order you see carousels in, right? By using all this information. So it's not as reliant on particular user data, but on how much more data they can use to create these experiences. ",
        "podNumber": 53,
        "clipNumber": 44
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think that paints such a compelling picture, especially because if you no longer need to have this treasure trove of data, right? Because the zero shot model from Vody can look at the picture and then extract all the, as you mentioned, like if it takes a picture of my living room, they'll see my other furniture and get a sense of my style, I suppose. Yeah, because it sounds like quite the business change. I think a lot about this thing of who should implement search, like should, you know, Alo that I mentioned earlier, should they be thinking about, we should control our own search on our website or should we use Amazon? I think in the past, you'd want to use Amazon or these big players because they had all this interaction data and that was like the only way to do it. But now because of these embedding models that, as you mentioned, can just take a picture of you and get a sense of your style instantly, do you think more brands will be thinking like, I'm cutting out the aggregation platform, I'm going to build this right into my website and manage this kind of advanced technology myself? ",
        "podNumber": 53,
        "clipNumber": 45
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yes, my answer to that would be two parts. One is like the distribution channel, the fact that they have the audience on some of these bigger platforms is sort of one piece. But when you flip to the side of being able to control more of their own on-site conversion, you know, 75% of revenue in e-commerce is lost from abandoned shopping carts. So if you don't know, you have duplicate products you're showing a user, they're gone. If they get confused about what they're looking at, they're gone, right? So I think being able to use all of their own data that we're using to make purchase decisions, to personalize an experience for us, they don't need to know as much about me anymore. They now have use of the data that I was using to make purchase decisions. So I think it can really transform how they personalize your home screen experience to how they show your objects. I mean, I imagine one day where they know I bought this shirt. So the next time I open skirts, I'm seeing it with this shirt. So now I'm imagining my own closet, right? Like you can get to that point where, so you too can kind of mix the personalization with what we were saying earlier, which is not needing it because you have such a great understanding of the product. So you don't need to know the product, the user as well. ",
        "podNumber": 53,
        "clipNumber": 46
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I agree. I hadn't thought about that before, but that is cool that it has your closet so it can orchestrate it that way, see what you're missing from the closet. And yeah, I mean, it's like, I'm curious about this kind of, it's more of a technical topic, but this kind of like diversity and recommendations and how embeddings achieves that, because these models are trained for similarity and to show you similar things to what you purchased. So how would a system work that shows you some things you, you know, like, because a lot of experiments with ref2vec, as I mentioned, like we're averaging the LeBron James, Kyrie Irving stuff, we average that vector. And so, you know, we're just going to get that. How do you think about diversity and recommendation with embeddings? ",
        "podNumber": 53,
        "clipNumber": 47
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yes. So think about like cart filling, like now you have an understanding about the product. You know, let's look at somebody like Home Depot or Lowe's, right? Now that you have an understanding, you don't want to show them more hammers or different hammers. You want to show them nails. You want to show them whatever gloves you'd use, whatever other tools you use with the hammer. Right. So some of that understanding. ",
        "podNumber": 53,
        "clipNumber": 48
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Yeah. I think you can get diversity from a lot of different ways. So I guess if we're talking just about embeddings and like these types of encoder only models, so to speak, you can achieve this by, for example, changing up dropout a little bit. You can sort of like maybe provide some sort of like noise context to these types of models, which I don't think is something people do, but it's entirely something that's like theoretically possible if you want to take the idea from like GANs. So that can help. You can also change the way that you're doing similarity. Maybe if you want to, you're taking the top 20 most similar things rather than going one, two, three, four, five, six, seven. You can sort of like switch up that order. A lot of people use like a bi-encoder, cross-encoder type framework and just changing up the way you use that might help adding more context. So you can create diversity recommendations simply by changing up your inputs, maybe changing up the model a little bit, or even like changing the way that you like post-process things. So getting that type of diversity is something that you can pretty much tinker with at pretty much every stage of this overall recommendation pipeline. And so ultimately I think that's something that's going to be up to say the individual users, like the companies that are integrating these models and these pipelines into their products. ",
        "podNumber": 53,
        "clipNumber": 49
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, Stephanie, you mentioned like baskets and Gunjan, you also mentioned this idea of like what the similarity function is capturing. I think that's quite an interesting idea because you know, initially when I was thinking about this, I was thinking usually with clip models, you like have an image and then you have its description and then you're trying to make that similar. But another way to train these models would be to optimize, like the positives come from they're in the same basket together. And then that way the function doesn't just like capture similarity, captures like this kind of like we have this relationship together, like we're in baskets together. But then I think with the basket, the training of the models is like, how would you kind of bootstrap the data for that without again, coming back to that problem where you need to be like Amazon and you have this massive collection of basket data? ",
        "podNumber": 53,
        "clipNumber": 50
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Sure. So the idea is you can use a larger model to try and improve sample efficiency. That would be one step. Now, if you're trying to compete with the billions of data Amazon has, you're probably going to have to be creative and try to like go back to like content-based space and try to like really richen up your content-based embeddings, incorporating multimodal will be a great step since these search recommendations, some people don't use that right now. Using multilingual, which as I've noticed is something that even Amazon struggles with, which was a really big surprise that I saw one month ago when I was exploring things. And so trying to build better content-based embeddings is a way if say you don't have Amazon's voluminous data in order to try and level up the playing field. That would be one step. And of course, by trying to figure out what type of information is valuable that way, if you have really good content recommendations, that itself is might drive some users and be able to help you sort of like get that interaction data that'll further improve your models. And so obviously it's going to be difficult to go up against Amazon, but most of these retailers that are like trying to build these sorts of models, maybe more niche or more specific to types of areas. And so they don't have to compete with Amazon everywhere. It doesn't matter what Amazon's pillow data is if you're selling bricks, for example, like that type of correlation simply isn't going to matter too much for your products. So if you focus on the things that you are trying to sell and try to get really good embeddings and content on that, and try to get interaction data on that specific set of products, and it'll be fine. Like if you're a boutique retailer that only like sells 35 different items, it doesn't really matter if like millions of people use Amazon. You just need a ton of data on those 35 products. And you're seeing a really understand your 35 products really well with your models and then you should be set. So I wouldn't worry too much about competing with Amazon unless you're literally trying to build the next Amazon, in which case, well, I don't know much about that. I can't really provide any advice.",
        "podNumber": 53,
        "clipNumber": 51
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, I think it, well, I think this is related to this concept of like, you know, the embedding models, they're between 20 million to 100 million parameters. And like being clever with the training techniques, like I'm sure at Vody, you know, you have a ton of expertise on like how you train these kinds of models. And that's a moat. And I think it's a very interesting kind of startup to pursue for people who've been studying machine learning. And the reason I'm saying this is we're talking about this kind of how you're doing the similarity, where you're getting the data from this kind of sampling of the positives and negatives. I think there's so much opportunity to innovate in how you sample positives and negatives and learn these kinds of models. And one idea I've been thinking about, and I'm curious if you have as well as like these kind of like graph neural networks where you have like user and then like, you know, like put it in a basket or, and then you also have a graph, like these items, they share the same brand. And that could be something as like a clever training technique, right? That maybe you'd uniquely offer. ",
        "podNumber": 53,
        "clipNumber": 52
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Yeah, it definitely is valuable. Graph networks have been valuable in trying to like use interaction data in particular, for example, since seeing the sequence of like the products you might use has been something that I think has historically been of interest. Transformers themselves can be considered some sort of graph networks and let the attention itself as some sort of graph, I do think is valuable, especially with interaction data. It's not something that I am particularly strong in, but it's definitely been something that I have been looking at. ",
        "podNumber": 53,
        "clipNumber": 53
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, super cool. I, I, yeah, I do think that with the graph nets, I guess, I don't know, like, cause I think a lot about we, we like, we was originally a knowledge graph technology. And so this, we still have these like cross references between the classes. And so I like to think a lot about how you can maybe send embeddings through the graph to get better embeddings for your particular context. That's kind of what inspires me to think about this kind of graph neural network thing. But so anyway, so I think it was a great coverage of, you know, getting into the meat of how the models are trained. Maybe kind of a wrapping up topic would be just like, what is on the horizon of multimodal that you're, you know, that's immediately in front of you as well as something that maybe is like three to five years out. ",
        "podNumber": 53,
        "clipNumber": 54
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "We'd have different timelines. ",
        "podNumber": 53,
        "clipNumber": 55
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "I think that having models that understand multimodal, like among text images, and then maybe domain specific modalities, like structured data, click stream videos, audio, what have you, that's going to be something that's going to be seen more and more across enterprise AI, maybe not necessarily academic since these types of things matter more from a business application. And so you'll see a lot more of that, at least the company's hoping for that. You'll probably also see a lot more of these types of multimodal models be used for generation capabilities. Definitely, there's going to be a lot of interest in the next few months, as long as this generative wave continues. But even when all the dust and hype settles, people will realize that being able to use all these different modalities and be able to generate text, like for example, if I don't know if Amazon or Wayfair want to build a chatbot to answer customer queries, something like multimodal is going to be very valuable for that. So those are the two areas that at least I think that multimodal is going to be very valuable for these types of companies in the next few years. We are expecting tons of adoption of this, and we expect it to completely turbocharge and change the way that people do recommendations. And maybe even off of the stretch, like something like Bing chat or a chatGPT plugin, people can simply interact with say, a chat and be like, ask for a product to be found. And then we can even automate product search so that rather than you having to sift through like 10 products and try to find the right thing, you can just talk to this model, and it'll just automatically be able to generate, say, the product that you're looking for, have a simple click button to like pay for it and one click, and then you're done. And so that I think is one particularly exciting application in multimodal LLMs, being able to use like a Bing chat type application in order to like automate large parts of product search so that, well, I mean, we already have models that can search the web better than a human can, at least that's the idea behind Bing chat. Why not have something like that that can search for e-commerce products better than a human can, and then probably save a lot of us so much time. So that's something, all those things are things I'm really excited about for the next few years. ",
        "podNumber": 53,
        "clipNumber": 56
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "I share his excitement on that side. I'd say what else I'm excited about is just the ability to actually get any of this done, right? So thank you to our startup friends like you guys, and OctoML offering inference, there's really going to be, it's going to be much easier and the cloud platforms and the investments they're making to do all this work, to use these models, to be able to use these embeddings and get them into production. And I think that's, in the next couple of years, that's one of the things I'm most excited about. ",
        "podNumber": 53,
        "clipNumber": 57
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. And so just one more thing, picking apart your brand, particularly Stephanie, being CEO and building businesses, I'm really curious how you see kind of the space of open source with this topic. If you think about, like, I'm curious how companies like yours, you know, offer this kind of like model training for particular companies, how you then also think about open sourcing some of the stuff you do and just how you see that. ",
        "podNumber": 53,
        "clipNumber": 58
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yeah. So super important topic to us. All this work is because of open source, right? So we embrace it. We love it. Right now we're working on sort of more limited open source, academic partnerships, places like that, so that we can make sure we're getting the appropriate amount of feedback and keeping it out there. I think our intention, you know, and as you said, how do things age? So maybe I'll regret this, but our intention would be that as we continue to evolve and release more models, we will make some of them open source. And we're working right now on how we're going to approach that. But for the interim, embracing it heavily because it's advancing our work. As we said, we're using a lot of open source models and using a lot of open source techniques to make these models work for e-commerce. And so we're going to find our way to give back to that. We have great data sets. They're enormous. That, for example, those types of partnerships, so we can help more of that open source work that we use get published, I would say is our first goal. ",
        "podNumber": 53,
        "clipNumber": 59
    },
    {
        "speaker": "Connor Shorten",
        "content": "That it's so fascinating, like the, the explosion of open source models. I mean, yeah, it's also interesting with kind of the idea of you could take one of these checkpoints from HuggingFace that comes from Vody, and then you could fine tune it, or maybe you already do that kind of thing. And I know talking to Brian about unstructured and how they're doing, how they're going to train a foundation models for like information extraction from say like PDFs and visual documents. This whole emerging space of the open source models is all so interesting. I have one more question on that, which is like there's open sourcing of the model weights themselves, but then there's also like, like something that I always like hat tip to MosaicML for and respect a lot, what they did is they open sourced this composer library. And so it's implementations of all sorts of efficient deep learning techniques that like, like alibi attention or like stochastic path dropout, like all these kinds of details of the models. What do you think about open sourcing some of the like, kind of like the, you know, the, the tricks of the training that goes in as well, or just kind of model weights. ",
        "podNumber": 53,
        "clipNumber": 60
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "I think there's room for both in that situation. Now, given that a lot of these innovations, alibi attention, rotary, well, I don't even know if rotary embeddings counts as an innovation at this point. So many models use it, but essentially I think a lot of that's going to be open source before we even like integrated to a model. Since a lot of the most valuable work in this space has been open source. OpenAI is basically just GPT-4 was like, let's go take everything that's open source available, add to GPT-3, and then make GPT-4 in essence. And they added some of their own stuff, but anything they added was very quickly open sourced by a lot of passionate researchers. And so we are going to say, open up that stuff. But by the time we do, my guess was be that there already are going to be multiple libraries out there that do that type of stuff. Like it wouldn't surprise me to say Phil Wang, Lucidrains, this crazy smart person who like open sources every model. I'm pretty sure that's still going to be the go-to place to find these types of innovations, no matter what, say, Vody or any other company does. Open source is always going to lead the way. And really any business is just going to try and combine those things to a model simply because there's tons of innovations, but they all tend to be like one specific area, one specific paper, one specific model. Combining those is where I think a lot of the moat and the opportunity lies. ",
        "podNumber": 53,
        "clipNumber": 61
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "And to your point Connor, our business very clearly sits in a lot of great open source work, not immediately applicable at a business level. And so our business is in taking a lot of this work and making sure that it is enterprise ready for business applications, which is a sort of extension of that. So I said, I think we'll probably end up contributing before that on how can we help advance some of the work that we then use. ",
        "podNumber": 53,
        "clipNumber": 62
    },
    {
        "speaker": "Connor Shorten",
        "content": "Yeah, fantastic. Awesome. Stephanie, Gunjan, that was such an incredible coverage of all these topics. I learned so much about training embedding models, all the things, how you're seeing the space. As we're wrapping up, do you maybe have any links, upcoming announcements, things that people listening can use to keep up with all the work you're doing? ",
        "podNumber": 53,
        "clipNumber": 63
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Yes, we are releasing a new web app where you can try our color model. They'll even do some data so that you don't have to prepare your own. And it will be coming on our new website, Vody.com. So definitely check it out. And you can apply to get a link to the API if you want.",
        "podNumber": 53,
        "clipNumber": 64
    },
    {
        "speaker": "Connor Shorten",
        "content": "Awesome. Thank you so much. ",
        "podNumber": 53,
        "clipNumber": 65
    },
    {
        "speaker": "Stephanie Horbaczewski",
        "content": "Thanks, Connor. Thanks so much. ",
        "podNumber": 53,
        "clipNumber": 66
    },
    {
        "speaker": "Gunjan Bhattarai",
        "content": "Thank you.",
        "podNumber": 53,
        "clipNumber": 67
    }
]